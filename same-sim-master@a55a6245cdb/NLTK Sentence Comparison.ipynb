{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "import math\n",
    "import pandas as pd\n",
    "CONST_PHI = 0.2\n",
    "CONST_BETA = 0.45\n",
    "CONST_ALPHA = 0.2\n",
    "CONST_PHI = 0.2\n",
    "CONST_DELTA = 0.875\n",
    "CONST_ETA = 0.4\n",
    "total_words = 0\n",
    "word_freq_brown = {}\n",
    "\n",
    "def Get_NewCodes():\n",
    "    df = pd.read_csv(\"Same_SIM_Draft.csv\")\n",
    "    sCodes = df['LongDesc'].tolist()\n",
    "    return sCodes; \n",
    "        \n",
    "def Get_MasterCodeDesc():\n",
    "    df = pd.read_csv(\"Same_SIM_Master_Draft.csv\")\n",
    "    sMasterCodeDesc = df['CPTLongDesc'].tolist()\n",
    "    return sMasterCodeDesc;\n",
    "\n",
    "\n",
    "def proper_synset(word_one , word_two):\n",
    "    pair = (None,None)\n",
    "    maximum_similarity = -1\n",
    "    synsets_one = wn.synsets(word_one)\n",
    "    synsets_two = wn.synsets(word_two)\n",
    "    #print(\"first word :\",word_one)\n",
    "    #print(\"second word\",word_two)\n",
    "    #print(synsets_one)\n",
    "    #print(synsets_two)\n",
    "    if(len(synsets_one)!=0 and len(synsets_two)!=0):\n",
    "        for synset_one in synsets_one:\n",
    "            for synset_two in synsets_two:\n",
    "                similarity = wn.path_similarity(synset_one,synset_two)\n",
    "                if(similarity == None):\n",
    "                    sim = -2\n",
    "                elif(similarity > maximum_similarity):\n",
    "                    maximum_similarity = similarity\n",
    "                    pair = synset_one,synset_two\n",
    "    else:\n",
    "        #need to see as for some word there will be no wordset.\n",
    "        #shuld make it as none\n",
    "        pair = (None , None)\n",
    "    return pair\n",
    "def length_between_words(synset_one , synset_two):\n",
    "    length = 100000000\n",
    "    if synset_one is None or synset_two is None:\n",
    "        return 0\n",
    "    elif(synset_one == synset_two):\n",
    "        length = 0\n",
    "    else:\n",
    "        words_synet1 = set([word.name() for word in synset_one.lemmas()])\n",
    "        words_synet2 = set([word.name() for word in synset_two.lemmas()])\n",
    "        if(len(words_synet1) + len(words_synet2) > len(words_synet1.union(words_synet2))):\n",
    "            length = 0\n",
    "        else:\n",
    "            #finding the actual distance\n",
    "            length = synset_one.shortest_path_distance(synset_two)\n",
    "            if(length is None):\n",
    "                return 0\n",
    "    return math.exp( -1 * CONST_ALPHA * length)\n",
    "def depth_common_subsumer(synset_one,synset_two):\n",
    "    height = 100000000\n",
    "    if synset_one is None or synset_two is None:\n",
    "        return 0\n",
    "    elif synset_one == synset_two:\n",
    "        height = max([hypernym[1] for hypernym in synset_one.hypernym_distances()])\n",
    "    else:\n",
    "        #get the hypernym set of both the synset.\n",
    "        hypernym_one = {hypernym_word[0]:hypernym_word[1] for hypernym_word in synset_one.hypernym_distances()}\n",
    "        hypernym_two = {hypernym_word[0]:hypernym_word[1] for hypernym_word in synset_two.hypernym_distances()}\n",
    "        common_subsumer = set(hypernym_one.keys()).intersection(set(hypernym_two.keys()))\n",
    "        if(len(common_subsumer) == 0):\n",
    "            height = 0\n",
    "        else:\n",
    "            height = 0\n",
    "            for cs in common_subsumer:\n",
    "                val = [hypernym_word[1] for hypernym_word in cs.hypernym_distances()]\n",
    "                val = max(val)\n",
    "                if val > height : height = val\n",
    "\n",
    "    #print(height) #works\n",
    "    return (math.exp(CONST_BETA * height) - math.exp(-CONST_BETA * height))/(math.exp(CONST_BETA * height) + math.exp(-CONST_BETA * height))\n",
    "def word_similarity(word1,word2):\n",
    "    #depth_common_subsumer(wn.synset('boy.n.01'),wn.synset('life_form.n.01'))\n",
    "    #print(wn.synset('boy.n.01').lowest_common_hypernym(wn.synset('animal.n.01')))\n",
    "    #print(wn.synset('boy.n.01').lowest_common_hypernym(wn.synset('girl.n.01')))\n",
    "    #word1 = input(\"Enter the first word: \")\n",
    "    #word2 = input(\"Enter the second word: \")\n",
    "    #synset_wordone = wn.synset(word1+\".n.01\")#doesnt work\n",
    "    #synset_wordtwo = wn.synset(word2+\".n.01\")#doesnt work\n",
    "    synset_wordone,synset_wordtwo = proper_synset(word1,word2) # cant just add +\".n.01\" to words to convert them to a synset.\n",
    "    #Need to execute the above as we cant know whether a 'noun' for of the word exists or not.\n",
    "    return length_between_words(synset_wordone,synset_wordtwo) * depth_common_subsumer(synset_wordone,synset_wordtwo)\n",
    "\n",
    "def I(search_word):\n",
    "    global total_words\n",
    "    if(total_words == 0):\n",
    "        for sent in brown.sents():\n",
    "            for word in sent:\n",
    "                word = word.lower()\n",
    "                if word not in word_freq_brown:\n",
    "                    word_freq_brown[word] = 0\n",
    "                word_freq_brown[word] +=1\n",
    "                total_words+=1\n",
    "    count = 0 if search_word not in word_freq_brown else word_freq_brown[search_word]\n",
    "    ret = 1.0 - (math.log(count+1)/math.log(total_words+1))\n",
    "    return ret\n",
    "def most_similar_word(word,sentence):\n",
    "    most_similarity = 0\n",
    "    most_similar_word = ''\n",
    "    for w in sentence:\n",
    "        #compute the word similarity using the already defined function\n",
    "        sim  =  word_similarity(w,word)\n",
    "        if sim > most_similarity:\n",
    "            most_similarity = sim\n",
    "            most_similar_word = w\n",
    "    if most_similarity <= CONST_PHI:\n",
    "        most_similarity = 0\n",
    "    return most_similar_word,most_similarity \n",
    "\n",
    "def gen_sem_vec(sentence , joint_word_set):\n",
    "    semantic_vector = np.zeros(len(joint_word_set))\n",
    "    #print(semantic_vector)\n",
    "    i = 0\n",
    "    #print(\"This is sentence :\",sentence)\n",
    "    #print(\"This is joint word set:\",joint_word_set)\n",
    "    for joint_word in joint_word_set:\n",
    "        sim_word = joint_word # to measure the \n",
    "        beta_sim_measure = 1\n",
    "        if (joint_word in sentence):\n",
    "            pass\n",
    "        else:\n",
    "            sim_word,beta_sim_measure = most_similar_word(joint_word,sentence) # gets the most similar word in that sentence.\n",
    "            beta_sim_measure = 0 if beta_sim_measure <= CONST_PHI else beta_sim_measure\n",
    "        sim_measure = beta_sim_measure * I(joint_word) * I(sim_word)\n",
    "        #sim_measure = beta_sim_measure ##Without information content which is got from the corpus.\n",
    "        semantic_vector[i] = sim_measure\n",
    "        i+=1\n",
    "    return semantic_vector\n",
    "def sent_sim(sent_set_one, sent_set_two , joint_word_set):\n",
    "    #sent_set_one = set(filter(lambda x : not (x == '.' or x == '?') , word_tokenize(sentence_one)))\n",
    "    #sent_set_two = set(filter(lambda x : not (x == '.' or x == '?') , word_tokenize(sentence_two)))\n",
    "    #print(sent_set_one)    \n",
    "    #print(sent_set_two)\n",
    "    #print(list(sent_set_one.union(sent_set_two)))\n",
    "    #joint_word_set = list(sent_set_one.union(sent_set_two))\n",
    "    #print(joint_word_set)\n",
    "    #sent_set_one = list(sent_set_one)\n",
    "    #sent_set_two = list(sent_set_two)\n",
    "    sem_vec_one = gen_sem_vec(sent_set_one,joint_word_set)\n",
    "    sem_vec_two = gen_sem_vec(sent_set_two,joint_word_set)\n",
    "    #multiply the two vectors..\n",
    "    #print(sem_vec_one)\n",
    "    #print(sem_vec_two)\n",
    "    return np.dot(sem_vec_one,sem_vec_two.T) / (np.linalg.norm(sem_vec_one) * np.linalg.norm(sem_vec_two))\n",
    "def word_order_similarity(sentence_one , sentence_two):\n",
    "    #print(\"Sentence one :\",sentence_one)\n",
    "    token_one  = word_tokenize(sentence_one)\n",
    "    #print(\"Sentence two : \",sentence_two)\n",
    "    token_two = word_tokenize(sentence_two)\n",
    "    joint_word_set = list(set(token_one).union(set(token_two)))\n",
    "    r1 = np.zeros(len(joint_word_set))\n",
    "    r2 = np.zeros(len(joint_word_set))\n",
    "    #filling for the first one\n",
    "    en_joint_one = {x[1]:x[0] for x in enumerate(token_one)}\n",
    "    en_joint_two = {x[1]:x[0] for x in enumerate(token_two)}\n",
    "    set_token_one = set(token_one)\n",
    "    set_token_two = set(token_two)\n",
    "    i = 0\n",
    "    #print(en_joint)\n",
    "    for word in joint_word_set:\n",
    "        if word in set_token_one:\n",
    "            r1[i] = en_joint_one[word]#so wrong.\n",
    "        else:\n",
    "            #get best word and check if its greater then a preset threshold\n",
    "            sim_word , sim = most_similar_word(word , list(set_token_one))\n",
    "            if sim > CONST_ETA : \n",
    "                r1[i] = en_joint_one[sim_word]\n",
    "            else:\n",
    "                r1[i] = 0\n",
    "        i+=1\n",
    "    j = 0\n",
    "    for word in joint_word_set:\n",
    "        if word in set_token_two:\n",
    "            r2[j] = en_joint_two[word]\n",
    "        else:\n",
    "            #get best word and check if its greater then a preset threshold\n",
    "            sim_word , sim = most_similar_word(word , list(set_token_two))\n",
    "            if sim > CONST_ETA : \n",
    "                r2[j] = en_joint_two[sim_word]\n",
    "            else:\n",
    "                r2[j] = 0\n",
    "        j+=1\n",
    "    return 1.0 - (np.linalg.norm(r1 - r2) / np.linalg.norm(r1 + r2))\n",
    "def main(sentence_one,sentence_two):\n",
    "    sent_set_one = set(filter(lambda x : not (x == '.' or x == '?') , word_tokenize(sentence_one)))\n",
    "    sent_set_two = set(filter(lambda x : not (x == '.' or x == '?') , word_tokenize(sentence_two)))\n",
    "    joint_word_set = list(sent_set_one.union(sent_set_two))\n",
    "    #Need to get the dictionary to have the corresponding indexes of the joint_word_set. \n",
    "    sentence_similarity = (CONST_DELTA * sent_sim(sent_set_one,sent_set_two,list(joint_word_set))) + ((1.0 - CONST_DELTA) * word_order_similarity(sentence_one,sentence_two))\n",
    "    return sentence_similarity\n",
    "#sentence_one = \"I play hockey\"\n",
    "#sentence_two = \"who are you?\"\n",
    "#print(main(sentence_one,sentence_two))\\\n",
    "def file_sem(f):\n",
    "    contents = open(f).read().strip()\n",
    "    ind_sentences = sent_tokenize(contents)\n",
    "    #print(ind_sentences)\n",
    "    no_of_sentences = len(ind_sentences)\n",
    "    sent_sim_matr = np.zeros((no_of_sentences,no_of_sentences))\n",
    "    i = 0\n",
    "    print(ind_sentences)\n",
    "    while(i < no_of_sentences):\n",
    "        j = i\n",
    "        while(j < no_of_sentences):\n",
    "            sent_sim_matr[i][j] = main(ind_sentences[i],ind_sentences[j])\n",
    "            sent_sim_matr[j][i] = sent_sim_matr[i][j]\n",
    "            j+=1\n",
    "        i+=1\n",
    "    return sent_sim_matr\n",
    "\n",
    "def intro():\n",
    "    sent_one = Get_NewCodes()\n",
    "    sent_two = Get_MasterCodeDesc()\n",
    "    for iNewLongDesc in sent_one:  \n",
    "            for iMasterDesc in sent_two:\n",
    "                    prob_sim_sent = main(iNewLongDesc , iMasterDesc)\n",
    "                    print(prob_sim_sent)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        intro()\n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
