{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0045U\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'softcossim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3caa79b39852>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mrowFound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresultFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresultFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNewCode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mNewCodeSet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mNewCodeNum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2083\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2084\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2085\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3caa79b39852>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;31m#tokenizer = TfidfVectorizer(stop_words = 'english')# initiate here your own tokenizer (TfidfVectorizer, CountVectorizer, with stopwords...)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_tokenizer_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_series\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_series\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;31m#score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"display.max_rows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-3caa79b39852>\u001b[0m in \u001b[0;36mcreate_tokenizer_score\u001b[1;34m(new_series, train_series, tokenizer)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mnew_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_series\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoftcossim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_tfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_series\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ix_new'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_series\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     score = pd.melt(\n",
      "\u001b[1;31mNameError\u001b[0m: name 'softcossim' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from nltk import word_tokenize\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "mylist = list()\n",
    "InputSheet = \"Same-Sim_SAMPLE.csv\"\n",
    "MasterSheet = \"Same_SIM_Master_List.csv\"\n",
    "resultFrame = pd.DataFrame(columns=('NewCode','NewCodeDesc','SameSimCode', 'CPTDesc','Accuracy','Rank','Comments'),index=range(30000))\n",
    "rownum = -1\n",
    "NewCodeNum = -1\n",
    "\n",
    "def create_tokenizer_score(new_series, train_series, tokenizer):\n",
    "    \"\"\"\n",
    "    return the tf idf score of each possible pairs of documents\n",
    "    Args:\n",
    "        new_series (pd.Series): new data (To compare against train data)\n",
    "        train_series (pd.Series): train data (To fit the tf-idf transformer)\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    train_tfidf = tokenizer.fit_transform(train_series)\n",
    "    \n",
    "    new_tfidf = tokenizer.transform(new_series)\n",
    "\n",
    "    X = pd.DataFrame(softcossim(new_tfidf, train_tfidf), columns=train_series.index)\n",
    "    X['ix_new'] = new_series.index\n",
    "    score = pd.melt(\n",
    "        X,\n",
    "        id_vars='ix_new',\n",
    "        var_name='ix_train',\n",
    "        value_name='score'\n",
    "    )\n",
    "    return score\n",
    "\n",
    "\n",
    "def Get_MasterCodeDesc():\n",
    "    df = pd.read_csv(MasterSheet)\n",
    "    sMasterCodeDesc = df['CPTLongDesc'].tolist()\n",
    "    return sMasterCodeDesc;\n",
    "\n",
    "def Get_SimCodeDesc():\n",
    "    df = pd.read_csv(InputSheet,encoding ='latin1')\n",
    "    sMasterCodeDesc = df['Sim_LongDesc'].tolist()\n",
    "    return sMasterCodeDesc;\n",
    "\n",
    "def GetNewCPT():\n",
    "    NewCodeFrame = pd.read_csv(InputSheet,encoding ='latin1')\n",
    "    sNewCode = NewCodeFrame['HCPCS'].tolist()\n",
    "    return sNewCode;\n",
    "\n",
    "def GetNewCPTDesc():\n",
    "    NewCodeFrame = pd.read_csv(InputSheet,encoding ='latin1')\n",
    "    sNewCodeDesc = NewCodeFrame['LongDesc'].tolist()\n",
    "    return sNewCodeDesc;\n",
    "\n",
    "def GetSameSIMData():\n",
    "    SameSimDf = pd.read_csv(InputSheet,encoding ='latin1')\n",
    "    sNewCodeDesc = SameSimDf['SimCode'].tolist()\n",
    "    return sNewCodeDesc;\n",
    "   \n",
    "def textblob_tokenizer(str_input):\n",
    "    blob = TextBlob(str_input.lower())\n",
    "    tokens = blob.words\n",
    "    words = [token.stem() for token in tokens]\n",
    "    return words\n",
    "\n",
    "def GetRowNum(): \n",
    "    try:\n",
    "        rownum = resultFrame.loc[(resultFrame.Accuracy == '0003T') & (resultFrame.Rank == 'CERVICOGRAPY')].index[0]\n",
    "            #print(rownum)\n",
    "        resultFrame.loc[rownum,'Comments'] = \"Test\"\n",
    "    except:\n",
    "        \"No record found\"\n",
    "\n",
    "def GetMasterCPTCode(Val,NewCode,NewCodeDesc,Accuracy,RankNo,Comment,x):\n",
    "    df = pd.read_csv(MasterSheet)    \n",
    "    GetCPTValue = df.iloc[Val,0]\n",
    "    GetCPTDesc = df.iloc[Val,1]    \n",
    "    mylist.append(GetCPTValue)\n",
    "    mylist.append(GetCPTDesc)\n",
    "    resultFrame.loc[x].NewCode = NewCode\n",
    "    resultFrame.loc[x].SameSimCode = GetCPTValue\n",
    "    resultFrame.loc[x].CPTDesc = GetCPTDesc\n",
    "    resultFrame.loc[x].NewCodeDesc = NewCodeDesc\n",
    "    resultFrame.loc[x].Accuracy = Accuracy\n",
    "    resultFrame.loc[x].Rank = RankNo\n",
    "    resultFrame.loc[x].Comments = Comment\n",
    "    return mylist;\n",
    "\n",
    "#train_CPT_Set = pd.Series(GetMasterCPT())\n",
    "train_set = pd.Series(Get_MasterCodeDesc())\n",
    "SimCodeDescSet = pd.Series(Get_SimCodeDesc())\n",
    "NewCodeDescSet = pd.Series(GetNewCPTDesc())\n",
    "NewCodeSet = pd.Series(GetNewCPT())\n",
    "SameSimSet = pd.Series(GetSameSIMData())\n",
    "for i in NewCodeDescSet:\n",
    "    classifier = GaussianNB()\n",
    "    NewCodeNum = NewCodeNum+1\n",
    "    test_set = pd.Series(i)   \n",
    "    try:\n",
    "        rowFound = resultFrame.loc[(resultFrame.NewCode == NewCodeSet[NewCodeNum])].index[0]  \n",
    "        try:        \n",
    "            rowNumSameSimFound = resultFrame.loc[(resultFrame.NewCode == NewCodeSet[NewCodeNum]) & (resultFrame.SameSimCode == SameSimSet[NewCodeNum])].index[0]\n",
    "            resultFrame.loc[rowNumSameSimFound,'Comments'] = \"Present Both in MD and Autoproposal\"\n",
    "        except:\n",
    "            try:\n",
    "                rownum = rownum+1\n",
    "                resultFrame.loc[rownum].NewCode = NewCodeSet[NewCodeNum]\n",
    "                resultFrame.loc[rownum].NewCodeDesc = NewCodeDescSet[NewCodeNum]\n",
    "                resultFrame.loc[rownum].SameSimCode = SameSimSet[NewCodeNum]\n",
    "                resultFrame.loc[rownum].CPTDesc = SimCodeDescSet[NewCodeNum]\n",
    "                resultFrame.loc[rownum].Comments = \"Present only by MD\"\n",
    "            except:\n",
    "                \"Nothing\"\n",
    "    except:\n",
    "        print(NewCodeSet[NewCodeNum])\n",
    "        #print(i)\n",
    "        test_set2 = word_tokenize(i)\n",
    "        #tokenizer = TfidfVectorizer(stop_words = 'english')# initiate here your own tokenizer (TfidfVectorizer, CountVectorizer, with stopwords...)\n",
    "        tokenizer = TfidfVectorizer()\n",
    "        score = create_tokenizer_score(train_series=train_set, new_series=test_set, tokenizer=tokenizer)\n",
    "        #score\n",
    "        pd.set_option(\"display.max_rows\", 11)\n",
    "        sortedscore = score.sort_values(by=['score'], ascending=False)\n",
    "        sortedscore.head(11)\n",
    "        ranknum = 0\n",
    "        found = \"false\"\n",
    "        for x in range(0,10):\n",
    "            ranknum = ranknum+1\n",
    "            df = pd.read_csv(MasterSheet) \n",
    "            svalue = sortedscore.iloc[x,1]\n",
    "            GetCPTValue = df.iloc[svalue,0]\n",
    "            if (GetCPTValue == SameSimSet[NewCodeNum]):\n",
    "                found = \"true\"\n",
    "                break \n",
    "\n",
    "        RecNum =0\n",
    "        for x in range(0,10):\n",
    "            rownum = rownum+1\n",
    "            svalue = sortedscore.iloc[x,1]\n",
    "            Accuracy = sortedscore.iloc[x,2]\n",
    "            \n",
    "            Comments = \"Present in Autoproposal\"\n",
    "\n",
    "            if (RecNum ==0):\n",
    "                result = GetMasterCPTCode(svalue,NewCodeSet[NewCodeNum],i,Accuracy*100,ranknum,Comments,rownum)\n",
    "            else:\n",
    "                result = GetMasterCPTCode(svalue,NewCodeSet[NewCodeNum],i,Accuracy*100,\"\",Comments,rownum)\n",
    "            RecNum =RecNum +1\n",
    "           \n",
    "        try:         \n",
    "            rowNumSameSimFound = resultFrame.loc[(resultFrame.NewCode == NewCodeSet[NewCodeNum]) & (resultFrame.SameSimCode == SameSimSet[NewCodeNum])].index[0]\n",
    "            resultFrame.loc[rowNumSameSimFound,'Comments'] = \"Present Both in MD and Autoproposal\"\n",
    "        except:\n",
    "            try:\n",
    "                rownum = rownum+1 \n",
    "                resultFrame.loc[rownum].NewCode = NewCodeSet[NewCodeNum]\n",
    "                resultFrame.loc[rownum].NewCodeDesc = NewCodeDescSet[NewCodeNum]\n",
    "                resultFrame.loc[rownum].SameSimCode = SameSimSet[NewCodeNum]\n",
    "                resultFrame.loc[rownum].CPTDesc = SimCodeDescSet[NewCodeNum]\n",
    "                resultFrame.loc[rownum].Comments = \"Present only by MD\"\n",
    "            except:\n",
    "                \"Nothing\"\n",
    "\n",
    "#export_csv = resultFrame.to_csv ('export_dataframe_V4_2018.csv', index = None, header=True)\n",
    "\n",
    "resultFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-169c1c9b27e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m    812\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Same-Sim_SAMPLE.csv', delimiter = '\\t', quoting = 3)\n",
    "\n",
    "# Cleaning the texts\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 1].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
