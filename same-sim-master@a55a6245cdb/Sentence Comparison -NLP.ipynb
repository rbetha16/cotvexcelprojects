{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): nltk in c:\\program files\\anaconda3\\lib\\site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 8.1.2, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial.n.02\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets(\"tEST\")\n",
    "print (syns[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Synset.lemmas of Synset('execute.v.01')>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets(\"Executed\")\n",
    "print (syns[0].lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lever that activates the firing mechanism of a gun\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordnet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3f13234757df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msyns1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ortho\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msyns1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wordnet' is not defined"
     ]
    }
   ],
   "source": [
    "syns1 = wordnet.synsets(\"ortho\")\n",
    "print (syns1[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of or relating to orthopedics\n"
     ]
    }
   ],
   "source": [
    "print (syns1[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orthopedic shoes']\n"
     ]
    }
   ],
   "source": [
    "print(syns1[0].examples())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oncology.n.01\n"
     ]
    }
   ],
   "source": [
    "syns2 = wordnet.synsets(\"oncology\")\n",
    "print (syns2[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-31e6ad68512b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msyns2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"oncology\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msyns2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "syns2 = wordnet.synsets(\"oncology\")\n",
    "print (syns2[1].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import semcor\n",
    "semcor.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quantitative'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "for syn in wordnet.synsets(\"quantitative\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        \n",
    "print (set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unorderable types: NoneType() > float()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-7e3ffe2f5320>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent_pair\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence_pairs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     print ( \n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0msimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_pair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_pair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         similarity(sent_pair[0], sent_pair[1], True))\n\u001b[1;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-7e3ffe2f5320>\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(sentence_1, sentence_2, info_content_norm)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mnormalization\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdesired\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \"\"\"\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mDELTA\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msemantic_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo_content_norm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mDELTA\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mword_order_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-7e3ffe2f5320>\u001b[0m in \u001b[0;36msemantic_similarity\u001b[0;34m(sentence_1, sentence_2, info_content_norm)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mwords_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mjoint_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mvec_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msemantic_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoint_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo_content_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m     \u001b[0mvec_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msemantic_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoint_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo_content_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvec_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec_1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-7e3ffe2f5320>\u001b[0m in \u001b[0;36msemantic_vector\u001b[0;34m(words, joint_words, info_content_norm)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[1;31m# find the most similar word in the joint set and set the sim value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0msim_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_sim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmost_similar_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoint_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0msemvec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPHI\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmax_sim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mPHI\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0minfo_content_norm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-7e3ffe2f5320>\u001b[0m in \u001b[0;36mmost_similar_word\u001b[0;34m(word, word_set)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0msim_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mref_word\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_set\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m       \u001b[0msim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0msim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmax_sim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m           \u001b[0mmax_sim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-7e3ffe2f5320>\u001b[0m in \u001b[0;36mword_similarity\u001b[0;34m(word_1, word_2)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mword_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0msynset_pair\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_best_synset_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     return (length_dist(synset_pair[0], synset_pair[1]) * \n\u001b[1;32m    106\u001b[0m         hierarchy_dist(synset_pair[0], synset_pair[1]))\n",
      "\u001b[0;32m<ipython-input-33-7e3ffe2f5320>\u001b[0m in \u001b[0;36mget_best_synset_pair\u001b[0;34m(word_1, word_2)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0msynset_2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msynsets_2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                \u001b[0msim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msynset_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msynset_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                \u001b[1;32mif\u001b[0m \u001b[0msim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmax_sim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                    \u001b[0mmax_sim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                    \u001b[0mbest_pair\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msynset_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msynset_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unorderable types: NoneType() > float()"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import brown\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Parameters to the algorithm. Currently set to values that was reported\n",
    "# in the paper to produce \"best\" results.\n",
    "ALPHA = 0.2\n",
    "BETA = 0.45\n",
    "ETA = 0.4\n",
    "PHI = 0.2\n",
    "DELTA = 0.85\n",
    "\n",
    "brown_freqs = dict()\n",
    "N = 0\n",
    "\n",
    "######################### word similarity ##########################\n",
    "\n",
    "def get_best_synset_pair(word_1, word_2):\n",
    "    \"\"\" \n",
    "    Choose the pair with highest path similarity among all pairs. \n",
    "    Mimics pattern-seeking behavior of humans.\n",
    "    \"\"\"\n",
    "    max_sim = -1.0\n",
    "    synsets_1 = wn.synsets(word_1)\n",
    "    synsets_2 = wn.synsets(word_2)\n",
    "    if len(synsets_1) == 0 or len(synsets_2) == 0:\n",
    "        return None, None\n",
    "    else:\n",
    "        max_sim = -1.0\n",
    "        best_pair = None, None\n",
    "        for synset_1 in synsets_1:\n",
    "            for synset_2 in synsets_2:\n",
    "               sim = wn.path_similarity(synset_1, synset_2)\n",
    "               if sim > max_sim:\n",
    "                   max_sim = sim\n",
    "                   best_pair = synset_1, synset_2\n",
    "        return best_pair\n",
    "\n",
    "def length_dist(synset_1, synset_2):\n",
    "    \"\"\"\n",
    "    Return a measure of the length of the shortest path in the semantic \n",
    "    ontology (Wordnet in our case as well as the paper's) between two \n",
    "    synsets.\n",
    "    \"\"\"\n",
    "    l_dist = sys.maxsize\n",
    "    if synset_1 is None or synset_2 is None: \n",
    "        return 0.0\n",
    "    if synset_1 == synset_2:\n",
    "        # if synset_1 and synset_2 are the same synset return 0\n",
    "        l_dist = 0.0\n",
    "    else:\n",
    "        wset_1 = set([str(x.name()) for x in synset_1.lemmas()])        \n",
    "        wset_2 = set([str(x.name()) for x in synset_2.lemmas()])\n",
    "        if len(wset_1.intersection(wset_2)) > 0:\n",
    "            # if synset_1 != synset_2 but there is word overlap, return 1.0\n",
    "            l_dist = 1.0\n",
    "        else:\n",
    "            # just compute the shortest path between the two\n",
    "            l_dist = synset_1.shortest_path_distance(synset_2)\n",
    "            if l_dist is None:\n",
    "                l_dist = 0.0\n",
    "    # normalize path length to the range [0,1]\n",
    "    return math.exp(-ALPHA * l_dist)\n",
    "\n",
    "def hierarchy_dist(synset_1, synset_2):\n",
    "    \"\"\"\n",
    "    Return a measure of depth in the ontology to model the fact that \n",
    "    nodes closer to the root are broader and have less semantic similarity\n",
    "    than nodes further away from the root.\n",
    "    \"\"\"\n",
    "    h_dist = sys.maxsize\n",
    "    if synset_1 is None or synset_2 is None: \n",
    "        return h_dist\n",
    "    if synset_1 == synset_2:\n",
    "        # return the depth of one of synset_1 or synset_2\n",
    "        h_dist = max([x[1] for x in synset_1.hypernym_distances()])\n",
    "    else:\n",
    "        # find the max depth of least common subsumer\n",
    "        hypernyms_1 = {x[0]:x[1] for x in synset_1.hypernym_distances()}\n",
    "        hypernyms_2 = {x[0]:x[1] for x in synset_2.hypernym_distances()}\n",
    "        lcs_candidates = set(hypernyms_1.keys()).intersection(\n",
    "            set(hypernyms_2.keys()))\n",
    "        if len(lcs_candidates) > 0:\n",
    "            lcs_dists = []\n",
    "            for lcs_candidate in lcs_candidates:\n",
    "                lcs_d1 = 0\n",
    "                if hypernyms_1.has_key(lcs_candidate):\n",
    "                    lcs_d1 = hypernyms_1[lcs_candidate]\n",
    "                lcs_d2 = 0\n",
    "                if hypernyms_2.has_key(lcs_candidate):\n",
    "                    lcs_d2 = hypernyms_2[lcs_candidate]\n",
    "                lcs_dists.append(max([lcs_d1, lcs_d2]))\n",
    "            h_dist = max(lcs_dists)\n",
    "        else:\n",
    "            h_dist = 0\n",
    "    return ((math.exp(BETA * h_dist) - math.exp(-BETA * h_dist)) / \n",
    "        (math.exp(BETA * h_dist) + math.exp(-BETA * h_dist)))\n",
    "    \n",
    "def word_similarity(word_1, word_2):\n",
    "    synset_pair = get_best_synset_pair(word_1, word_2)\n",
    "    return (length_dist(synset_pair[0], synset_pair[1]) * \n",
    "        hierarchy_dist(synset_pair[0], synset_pair[1]))\n",
    "\n",
    "######################### sentence similarity ##########################\n",
    "\n",
    "def most_similar_word(word, word_set):\n",
    "    \"\"\"\n",
    "    Find the word in the joint word set that is most similar to the word\n",
    "    passed in. We use the algorithm above to compute word similarity between\n",
    "    the word and each word in the joint word set, and return the most similar\n",
    "    word and the actual similarity value.\n",
    "    \"\"\"\n",
    "    max_sim = -1.0\n",
    "    sim_word = \"\"\n",
    "    for ref_word in word_set:\n",
    "      sim = word_similarity(word, ref_word)\n",
    "      if sim > max_sim:\n",
    "          max_sim = sim\n",
    "          sim_word = ref_word\n",
    "    return sim_word, max_sim\n",
    "    \n",
    "def info_content(lookup_word):\n",
    "    \"\"\"\n",
    "    Uses the Brown corpus available in NLTK to calculate a Laplace\n",
    "    smoothed frequency distribution of words, then uses this information\n",
    "    to compute the information content of the lookup_word.\n",
    "    \"\"\"\n",
    "    global N\n",
    "    if N == 0:\n",
    "        # poor man's lazy evaluation\n",
    "        for sent in brown.sents():\n",
    "            for word in sent:\n",
    "                word = word.lower()\n",
    "                if not brown_freqs.has_key(word):\n",
    "                    brown_freqs[word] = 0\n",
    "                brown_freqs[word] = brown_freqs[word] + 1\n",
    "                N = N + 1\n",
    "    lookup_word = lookup_word.lower()\n",
    "    n = 0 if not brown_freqs.has_key(lookup_word) else brown_freqs[lookup_word]\n",
    "    return 1.0 - (math.log(n + 1) / math.log(N + 1))\n",
    "    \n",
    "def semantic_vector(words, joint_words, info_content_norm):\n",
    "    \"\"\"\n",
    "    Computes the semantic vector of a sentence. The sentence is passed in as\n",
    "    a collection of words. The size of the semantic vector is the same as the\n",
    "    size of the joint word set. The elements are 1 if a word in the sentence\n",
    "    already exists in the joint word set, or the similarity of the word to the\n",
    "    most similar word in the joint word set if it doesn't. Both values are \n",
    "    further normalized by the word's (and similar word's) information content\n",
    "    if info_content_norm is True.\n",
    "    \"\"\"\n",
    "    sent_set = set(words)\n",
    "    semvec = np.zeros(len(joint_words))\n",
    "    i = 0\n",
    "    for joint_word in joint_words:\n",
    "        if joint_word in sent_set:\n",
    "            # if word in union exists in the sentence, s(i) = 1 (unnormalized)\n",
    "            semvec[i] = 1.0\n",
    "            if info_content_norm:\n",
    "                semvec[i] = semvec[i] * math.pow(info_content(joint_word), 2)\n",
    "        else:\n",
    "            # find the most similar word in the joint set and set the sim value\n",
    "            sim_word, max_sim = most_similar_word(joint_word, sent_set)\n",
    "            semvec[i] = PHI if max_sim > PHI else 0.0\n",
    "            if info_content_norm:\n",
    "                semvec[i] = semvec[i] * info_content(joint_word) * info_content(sim_word)\n",
    "        i = i + 1\n",
    "    return semvec                \n",
    "            \n",
    "def semantic_similarity(sentence_1, sentence_2, info_content_norm):\n",
    "    \"\"\"\n",
    "    Computes the semantic similarity between two sentences as the cosine\n",
    "    similarity between the semantic vectors computed for each sentence.\n",
    "    \"\"\"\n",
    "    words_1 = nltk.word_tokenize(sentence_1)\n",
    "    words_2 = nltk.word_tokenize(sentence_2)\n",
    "    joint_words = set(words_1).union(set(words_2))\n",
    "    vec_1 = semantic_vector(words_1, joint_words, info_content_norm)\n",
    "    vec_2 = semantic_vector(words_2, joint_words, info_content_norm)\n",
    "    return np.dot(vec_1, vec_2.T) / (np.linalg.norm(vec_1) * np.linalg.norm(vec_2))\n",
    "\n",
    "######################### word order similarity ##########################\n",
    "\n",
    "def word_order_vector(words, joint_words, windex):\n",
    "    \"\"\"\n",
    "    Computes the word order vector for a sentence. The sentence is passed\n",
    "    in as a collection of words. The size of the word order vector is the\n",
    "    same as the size of the joint word set. The elements of the word order\n",
    "    vector are the position mapping (from the windex dictionary) of the \n",
    "    word in the joint set if the word exists in the sentence. If the word\n",
    "    does not exist in the sentence, then the value of the element is the \n",
    "    position of the most similar word in the sentence as long as the similarity\n",
    "    is above the threshold ETA.\n",
    "    \"\"\"\n",
    "    wovec = np.zeros(len(joint_words))\n",
    "    i = 0\n",
    "    wordset = set(words)\n",
    "    for joint_word in joint_words:\n",
    "        if joint_word in wordset:\n",
    "            # word in joint_words found in sentence, just populate the index\n",
    "            wovec[i] = windex[joint_word]\n",
    "        else:\n",
    "            # word not in joint_words, find most similar word and populate\n",
    "            # word_vector with the thresholded similarity\n",
    "            sim_word, max_sim = most_similar_word(joint_word, wordset)\n",
    "            if max_sim > ETA:\n",
    "                wovec[i] = windex[sim_word]\n",
    "            else:\n",
    "                wovec[i] = 0\n",
    "        i = i + 1\n",
    "    return wovec\n",
    "\n",
    "def word_order_similarity(sentence_1, sentence_2):\n",
    "    \"\"\"\n",
    "    Computes the word-order similarity between two sentences as the normalized\n",
    "    difference of word order between the two sentences.\n",
    "    \"\"\"\n",
    "    words_1 = nltk.word_tokenize(sentence_1)\n",
    "    words_2 = nltk.word_tokenize(sentence_2)\n",
    "    joint_words = list(set(words_1).union(set(words_2)))\n",
    "    windex = {x[1]: x[0] for x in enumerate(joint_words)}\n",
    "    r1 = word_order_vector(words_1, joint_words, windex)\n",
    "    r2 = word_order_vector(words_2, joint_words, windex)\n",
    "    return 1.0 - (np.linalg.norm(r1 - r2) / np.linalg.norm(r1 + r2))\n",
    "\n",
    "######################### overall similarity ##########################\n",
    "\n",
    "def similarity(sentence_1, sentence_2, info_content_norm):\n",
    "    \"\"\"\n",
    "    Calculate the semantic similarity between two sentences. The last \n",
    "    parameter is True or False depending on whether information content\n",
    "    normalization is desired or not.\n",
    "    \"\"\"\n",
    "    return DELTA * semantic_similarity(sentence_1, sentence_2, info_content_norm) (1.0 - DELTA) * word_order_similarity(sentence_1, sentence_2)\n",
    "\n",
    "\n",
    "sentence_pairs = [\n",
    "    [\"I like that bachelor.\", \"I like that unmarried man.\", 0.561],\n",
    "    [\"John is very nice.\", \"Is John very nice?\", 0.977],\n",
    "    [\"Red alcoholic drink.\", \"A bottle of wine.\", 0.585],\n",
    "    [\"Red alcoholic drink.\", \"Fresh orange juice.\", 0.611],\n",
    "    [\"Red alcoholic drink.\", \"An English dictionary.\", 0.0],\n",
    "    [\"Red alcoholic drink.\", \"Fresh apple juice.\", 0.420],\n",
    "    [\"A glass of cider.\", \"A full cup of apple juice.\", 0.678],\n",
    "    [\"It is a dog.\", \"That must be your dog.\", 0.739],\n",
    "    [\"It is a dog.\", \"It is a log.\", 0.623],\n",
    "    [\"It is a dog.\", \"It is a pig.\", 0.790],\n",
    "    [\"Dogs are animals.\", \"They are common pets.\", 0.738],\n",
    "    [\"Canis familiaris are animals.\", \"Dogs are common pets.\", 0.362],\n",
    "    [\"I have a pen.\", \"Where do you live?\", 0.0],\n",
    "    [\"I have a pen.\", \"Where is ink?\", 0.129],\n",
    "    [\"I have a hammer.\", \"Take some nails.\", 0.508],\n",
    "    [\"I have a hammer.\", \"Take some apples.\", 0.121]\n",
    "]\n",
    "for sent_pair in sentence_pairs:\n",
    "    print ( \n",
    "        similarity(sent_pair[0], sent_pair[1], False),\n",
    "        similarity(sent_pair[0], sent_pair[1], True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-498946147afe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msentence_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"He is well\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"I am suffering\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-498946147afe>\u001b[0m in \u001b[0;36msentence_similarity\u001b[1;34m(sentence1, sentence2)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msynset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msynsets1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# Get the similarity value of the most similar word in the other sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mbest_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msynset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mss\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msynsets2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m# Check that the similarity could have been computed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    " \n",
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    " \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    " \n",
    "    return None\n",
    " \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    " \n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    " \n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    " \n",
    "    score, count = 0.0, 0.0\n",
    " \n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        best_score = max([synset.path_similarity(ss) for ss in synsets2])\n",
    " \n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score += best_score\n",
    "            count += 1\n",
    " \n",
    "    # Average the values\n",
    "    score /= count\n",
    "    return score\n",
    " \n",
    "print (sentence_similarity(\"He is well\",\"I am suffering\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Sentence Similarity--------------------------\n",
      "\n",
      "Enter a valid option:\n",
      "\n",
      "1.Sentence Similarity between two files containing different sentences.\n",
      "2.Sentence similarity between two sentences\n",
      "\n",
      "Your choice : 1\n",
      "Enter the path of the file :1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-da143233f628>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-------------------Sentence Similarity--------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0mintro\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Want to try once again? if yes press 1 or else 0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0mexcited\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-da143233f628>\u001b[0m in \u001b[0;36mintro\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mfile_one\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Enter the path of the file :\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[1;31m#file_two = input(\"Enter the path of the second file\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mprob_sim_sent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_sem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_one\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Similarity between the sentences in a single file is : (IN MATRIX FORM)\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_sim_sent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-da143233f628>\u001b[0m in \u001b[0;36mfile_sem\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[1;31m#print(main(sentence_one,sentence_two))\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfile_sem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mcontents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0mind_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[1;31m#print(ind_sentences)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1'"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "import math\n",
    "import Panda Utils\n",
    "CONST_PHI = 0.2\n",
    "CONST_BETA = 0.45\n",
    "CONST_ALPHA = 0.2\n",
    "CONST_PHI = 0.2\n",
    "CONST_DELTA = 0.875\n",
    "CONST_ETA = 0.4\n",
    "total_words = 0\n",
    "word_freq_brown = {}\n",
    "def proper_synset(word_one , word_two):\n",
    "    pair = (None,None)\n",
    "    maximum_similarity = -1\n",
    "    synsets_one = wn.synsets(word_one)\n",
    "    synsets_two = wn.synsets(word_two)\n",
    "    #print(\"first word :\",word_one)\n",
    "    #print(\"second word\",word_two)\n",
    "    #print(synsets_one)\n",
    "    #print(synsets_two)\n",
    "    if(len(synsets_one)!=0 and len(synsets_two)!=0):\n",
    "        for synset_one in synsets_one:\n",
    "            for synset_two in synsets_two:\n",
    "                similarity = wn.path_similarity(synset_one,synset_two)\n",
    "                if(similarity == None):\n",
    "                    sim = -2\n",
    "                elif(similarity > maximum_similarity):\n",
    "                    maximum_similarity = similarity\n",
    "                    pair = synset_one,synset_two\n",
    "    else:\n",
    "        #need to see as for some word there will be no wordset.\n",
    "        #shuld make it as none\n",
    "        pair = (None , None)\n",
    "    return pair\n",
    "def length_between_words(synset_one , synset_two):\n",
    "    length = 100000000\n",
    "    if synset_one is None or synset_two is None:\n",
    "        return 0\n",
    "    elif(synset_one == synset_two):\n",
    "        length = 0\n",
    "    else:\n",
    "        words_synet1 = set([word.name() for word in synset_one.lemmas()])\n",
    "        words_synet2 = set([word.name() for word in synset_two.lemmas()])\n",
    "        if(len(words_synet1) + len(words_synet2) > len(words_synet1.union(words_synet2))):\n",
    "            length = 0\n",
    "        else:\n",
    "            #finding the actual distance\n",
    "            length = synset_one.shortest_path_distance(synset_two)\n",
    "            if(length is None):\n",
    "                return 0\n",
    "    return math.exp( -1 * CONST_ALPHA * length)\n",
    "def depth_common_subsumer(synset_one,synset_two):\n",
    "    height = 100000000\n",
    "    if synset_one is None or synset_two is None:\n",
    "        return 0\n",
    "    elif synset_one == synset_two:\n",
    "        height = max([hypernym[1] for hypernym in synset_one.hypernym_distances()])\n",
    "    else:\n",
    "        #get the hypernym set of both the synset.\n",
    "        hypernym_one = {hypernym_word[0]:hypernym_word[1] for hypernym_word in synset_one.hypernym_distances()}\n",
    "        hypernym_two = {hypernym_word[0]:hypernym_word[1] for hypernym_word in synset_two.hypernym_distances()}\n",
    "        common_subsumer = set(hypernym_one.keys()).intersection(set(hypernym_two.keys()))\n",
    "        if(len(common_subsumer) == 0):\n",
    "            height = 0\n",
    "        else:\n",
    "            height = 0\n",
    "            for cs in common_subsumer:\n",
    "                val = [hypernym_word[1] for hypernym_word in cs.hypernym_distances()]\n",
    "                val = max(val)\n",
    "                if val > height : height = val\n",
    "\n",
    "    #print(height) #works\n",
    "    return (math.exp(CONST_BETA * height) - math.exp(-CONST_BETA * height))/(math.exp(CONST_BETA * height) + math.exp(-CONST_BETA * height))\n",
    "def word_similarity(word1,word2):\n",
    "    #depth_common_subsumer(wn.synset('boy.n.01'),wn.synset('life_form.n.01'))\n",
    "    #print(wn.synset('boy.n.01').lowest_common_hypernym(wn.synset('animal.n.01')))\n",
    "    #print(wn.synset('boy.n.01').lowest_common_hypernym(wn.synset('girl.n.01')))\n",
    "    #word1 = input(\"Enter the first word: \")\n",
    "    #word2 = input(\"Enter the second word: \")\n",
    "    #synset_wordone = wn.synset(word1+\".n.01\")#doesnt work\n",
    "    #synset_wordtwo = wn.synset(word2+\".n.01\")#doesnt work\n",
    "    synset_wordone,synset_wordtwo = proper_synset(word1,word2) # cant just add +\".n.01\" to words to convert them to a synset.\n",
    "    #Need to execute the above as we cant know whether a 'noun' for of the word exists or not.\n",
    "    return length_between_words(synset_wordone,synset_wordtwo) * depth_common_subsumer(synset_wordone,synset_wordtwo)\n",
    "\n",
    "def I(search_word):\n",
    "    global total_words\n",
    "    if(total_words == 0):\n",
    "        for sent in brown.sents():\n",
    "            for word in sent:\n",
    "                word = word.lower()\n",
    "                if word not in word_freq_brown:\n",
    "                    word_freq_brown[word] = 0\n",
    "                word_freq_brown[word] +=1\n",
    "                total_words+=1\n",
    "    count = 0 if search_word not in word_freq_brown else word_freq_brown[search_word]\n",
    "    ret = 1.0 - (math.log(count+1)/math.log(total_words+1))\n",
    "    return ret\n",
    "def most_similar_word(word,sentence):\n",
    "    most_similarity = 0\n",
    "    most_similar_word = ''\n",
    "    for w in sentence:\n",
    "        #compute the word similarity using the already defined function\n",
    "        sim  =  word_similarity(w,word)\n",
    "        if sim > most_similarity:\n",
    "            most_similarity = sim\n",
    "            most_similar_word = w\n",
    "    if most_similarity <= CONST_PHI:\n",
    "        most_similarity = 0\n",
    "    return most_similar_word,most_similarity \n",
    "\n",
    "def gen_sem_vec(sentence , joint_word_set):\n",
    "    semantic_vector = np.zeros(len(joint_word_set))\n",
    "    #print(semantic_vector)\n",
    "    i = 0\n",
    "    #print(\"This is sentence :\",sentence)\n",
    "    #print(\"This is joint word set:\",joint_word_set)\n",
    "    for joint_word in joint_word_set:\n",
    "        sim_word = joint_word # to measure the \n",
    "        beta_sim_measure = 1\n",
    "        if (joint_word in sentence):\n",
    "            pass\n",
    "        else:\n",
    "            sim_word,beta_sim_measure = most_similar_word(joint_word,sentence) # gets the most similar word in that sentence.\n",
    "            beta_sim_measure = 0 if beta_sim_measure <= CONST_PHI else beta_sim_measure\n",
    "        sim_measure = beta_sim_measure * I(joint_word) * I(sim_word)\n",
    "        #sim_measure = beta_sim_measure ##Without information content which is got from the corpus.\n",
    "        semantic_vector[i] = sim_measure\n",
    "        i+=1\n",
    "    return semantic_vector\n",
    "def sent_sim(sent_set_one, sent_set_two , joint_word_set):\n",
    "    #sent_set_one = set(filter(lambda x : not (x == '.' or x == '?') , word_tokenize(sentence_one)))\n",
    "    #sent_set_two = set(filter(lambda x : not (x == '.' or x == '?') , word_tokenize(sentence_two)))\n",
    "    #print(sent_set_one)    \n",
    "    #print(sent_set_two)\n",
    "    #print(list(sent_set_one.union(sent_set_two)))\n",
    "    #joint_word_set = list(sent_set_one.union(sent_set_two))\n",
    "    #print(joint_word_set)\n",
    "    #sent_set_one = list(sent_set_one)\n",
    "    #sent_set_two = list(sent_set_two)\n",
    "    sem_vec_one = gen_sem_vec(sent_set_one,joint_word_set)\n",
    "    sem_vec_two = gen_sem_vec(sent_set_two,joint_word_set)\n",
    "    #multiply the two vectors..\n",
    "    #print(sem_vec_one)\n",
    "    #print(sem_vec_two)\n",
    "    return np.dot(sem_vec_one,sem_vec_two.T) / (np.linalg.norm(sem_vec_one) * np.linalg.norm(sem_vec_two))\n",
    "def word_order_similarity(sentence_one , sentence_two):\n",
    "    #print(\"Sentence one :\",sentence_one)\n",
    "    token_one  = word_tokenize(sentence_one)\n",
    "    #print(\"Sentence two : \",sentence_two)\n",
    "    token_two = word_tokenize(sentence_two)\n",
    "    joint_word_set = list(set(token_one).union(set(token_two)))\n",
    "    r1 = np.zeros(len(joint_word_set))\n",
    "    r2 = np.zeros(len(joint_word_set))\n",
    "    #filling for the first one\n",
    "    en_joint_one = {x[1]:x[0] for x in enumerate(token_one)}\n",
    "    en_joint_two = {x[1]:x[0] for x in enumerate(token_two)}\n",
    "    set_token_one = set(token_one)\n",
    "    set_token_two = set(token_two)\n",
    "    i = 0\n",
    "    #print(en_joint)\n",
    "    for word in joint_word_set:\n",
    "        if word in set_token_one:\n",
    "            r1[i] = en_joint_one[word]#so wrong.\n",
    "        else:\n",
    "            #get best word and check if its greater then a preset threshold\n",
    "            sim_word , sim = most_similar_word(word , list(set_token_one))\n",
    "            if sim > CONST_ETA : \n",
    "                r1[i] = en_joint_one[sim_word]\n",
    "            else:\n",
    "                r1[i] = 0\n",
    "        i+=1\n",
    "    j = 0\n",
    "    for word in joint_word_set:\n",
    "        if word in set_token_two:\n",
    "            r2[j] = en_joint_two[word]\n",
    "        else:\n",
    "            #get best word and check if its greater then a preset threshold\n",
    "            sim_word , sim = most_similar_word(word , list(set_token_two))\n",
    "            if sim > CONST_ETA : \n",
    "                r2[j] = en_joint_two[sim_word]\n",
    "            else:\n",
    "                r2[j] = 0\n",
    "        j+=1\n",
    "    return 1.0 - (np.linalg.norm(r1 - r2) / np.linalg.norm(r1 + r2))\n",
    "def main(sentence_one,sentence_two):\n",
    "    sent_set_one = set(filter(lambda x : not (x == '.' or x == '?') , word_tokenize(sentence_one)))\n",
    "    sent_set_two = set(filter(lambda x : not (x == '.' or x == '?') , word_tokenize(sentence_two)))\n",
    "    joint_word_set = list(sent_set_one.union(sent_set_two))\n",
    "    #Need to get the dictionary to have the corresponding indexes of the joint_word_set. \n",
    "    sentence_similarity = (CONST_DELTA * sent_sim(sent_set_one,sent_set_two,list(joint_word_set))) + ((1.0 - CONST_DELTA) * word_order_similarity(sentence_one,sentence_two))\n",
    "    return sentence_similarity\n",
    "#sentence_one = \"I play hockey\"\n",
    "#sentence_two = \"who are you?\"\n",
    "#print(main(sentence_one,sentence_two))\\\n",
    "def file_sem(f):\n",
    "    contents = open(f).read().strip()\n",
    "    ind_sentences = sent_tokenize(contents)\n",
    "    #print(ind_sentences)\n",
    "    no_of_sentences = len(ind_sentences)\n",
    "    sent_sim_matr = np.zeros((no_of_sentences,no_of_sentences))\n",
    "    i = 0\n",
    "    print(ind_sentences)\n",
    "    while(i < no_of_sentences):\n",
    "        j = i\n",
    "        while(j < no_of_sentences):\n",
    "            sent_sim_matr[i][j] = main(ind_sentences[i],ind_sentences[j])\n",
    "            sent_sim_matr[j][i] = sent_sim_matr[i][j]\n",
    "            j+=1\n",
    "        i+=1\n",
    "    return sent_sim_matr\n",
    "def intro():\n",
    "    print(\"\\nEnter a valid option:\\n\")\n",
    "    print(\"1.Sentence Similarity between two files containing different sentences.\")\n",
    "    print(\"2.Sentence similarity between two sentences\\n\")\n",
    "    option = int(input(\"Your choice : \"))\n",
    "    if option == 1:\n",
    "        file_one = input(\"Enter the path of the file :\")\n",
    "        #file_two = input(\"Enter the path of the second file\")\n",
    "        prob_sim_sent = file_sem(file_one)\n",
    "        print(\"Similarity between the sentences in a single file is : (IN MATRIX FORM)\\n\")\n",
    "        print(prob_sim_sent)\n",
    "        #could've pickeled , but wrote it to a file.\n",
    "        f_n = file_one[0:len(file_one)-4:]+\"_matrix.txt\"\n",
    "        output_file = open(f_n,'w')\n",
    "        output_file.write(str(prob_sim_sent))\n",
    "    elif option == 2:\n",
    "        sent_one = input(\"Enter the first sentence : \")\n",
    "        sent_two = input(\"Enter the second sentence two :\")\n",
    "        prob_sim_sent = main(sent_one , sent_two)\n",
    "        print(prob_sim_sent)\n",
    "        #print(\"Similarity between\\n\"+sent_one+\"\\n\"+sent_two+\"\\n\\n is : \",prob_sim_sent)\n",
    "    else:\n",
    "        global max_count\n",
    "        if max_count < 3 : print(\"Wrong Choice Try again\"); max_count+=1 \n",
    "        else: print(\"Wrong choice time exceeded!\");exit()\n",
    "        intro()\n",
    "if __name__ == \"__main__\":  \n",
    "    print(\"-------------------Sentence Similarity--------------------------\")\n",
    "    intro()\n",
    "    print(\"Want to try once again? if yes press 1 or else 0\")\n",
    "    excited = int(input())\n",
    "    while(excited == 1):\n",
    "        intro()\n",
    "        print(\"Want to try once again?\")\n",
    "        excited = int(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Using cached https://files.pythonhosted.org/packages/46/dc/7fd5df840efb3e56c8b4f768793a237ec4ee59891959d6a215d63f727023/pip-19.0.1-py2.py3-none-any.whl\n",
      "Collecting user\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Could not find a version that satisfies the requirement user (from versions: )\n",
      "No matching distribution found for user\n",
      "You are using pip version 10.0.1, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9f9f179f16aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def load_sts_dataset(filename):\n",
    "    \"\"\"\n",
    "     Loads a subset of the STS dataset into a DataFrame.\n",
    "     In particular both sentences and their human rated similarity score.\n",
    "    :param filename:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sent_pairs = []\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            ts = line.strip().split(\"\\t\")\n",
    "            sent_pairs.append((ts[5], ts[6], float(ts[4])))\n",
    "    return pd.DataFrame(sent_pairs, columns=[\"sent_1\", \"sent_2\", \"sim\"])\n",
    "\n",
    "\n",
    "def download_and_load_sts_data():\n",
    "    sts_dataset = tf.keras.utils.get_file(\n",
    "        fname=\"mesh.nt.gz\",\n",
    "        origin=\"ftp://ftp.nlm.nih.gov/online/mesh/rdf/mesh.nt.gz\",\n",
    "        extract=True)\n",
    "\n",
    "    sts_dev = load_sts_dataset(os.path.join(os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-dev.csv\"))\n",
    "    sts_test = load_sts_dataset(os.path.join(os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-test.csv\"))\n",
    "\n",
    "    return sts_dev, sts_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Sentence Similarity--------------------------\n",
      "\n",
      "Enter a valid option:\n",
      "\n",
      "1.Sentence Similarity between two files containing different sentences.\n",
      "2.Sentence similarity between two sentences\n",
      "\n",
      "Your choice : 2\n",
      "Enter the first sentence : hi this is ravi\n",
      "Enter the second sentence two :hi whoese is ravi\n",
      "(['hi', 'this', 'is', 'ravi'], ['hi', 'whoese', 'is', 'ravi'])\n",
      "Want to try once again? if yes press 1 or else 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def tokenize(q1, q2):\n",
    "    \"\"\"\n",
    "        q1 and q2 are sentences/questions. Function returns a list of tokens for both.\n",
    "    \"\"\"\n",
    "    return word_tokenize(q1), word_tokenize(q2)\n",
    "\n",
    "\n",
    "def posTag(q1, q2):\n",
    "    \"\"\"\n",
    "        q1 and q2 are lists. Function returns a list of POS tagged tokens for both.\n",
    "    \"\"\"\n",
    "    return nltk.pos_tag(q1), nltk.pos_tag(q2)\n",
    "\n",
    "\n",
    "def stemmer(tag_q1, tag_q2):\n",
    "    \"\"\"\n",
    "        tag_q = tagged lists. Function returns a stemmed list.\n",
    "    \"\"\"\n",
    "\n",
    "    stem_q1 = []\n",
    "    stem_q2 = []\n",
    "\n",
    "    for token in tag_q1:\n",
    "        stem_q1.append(stem(token))\n",
    "\n",
    "    for token in tag_q2:\n",
    "        stem_q2.append(stem(token))\n",
    "\n",
    "    return stem_q1, stem_q2\n",
    "\n",
    "def intro():\n",
    "    print(\"\\nEnter a valid option:\\n\")\n",
    "    print(\"1.Sentence Similarity between two files containing different sentences.\")\n",
    "    print(\"2.Sentence similarity between two sentences\\n\")\n",
    "    option = int(input(\"Your choice : \"))\n",
    "    if option == 1:\n",
    "        file_one = input(\"Enter the path of the file :\")\n",
    "        #file_two = input(\"Enter the path of the second file\")\n",
    "        prob_sim_sent = word_tokenize(file_one)\n",
    "        print(\"Similarity between the sentences in a single file is : (IN MATRIX FORM)\\n\")\n",
    "        print(prob_sim_sent)\n",
    "        #could've pickeled , but wrote it to a file.\n",
    "        f_n = file_one[0:len(file_one)-4:]+\"_matrix.txt\"\n",
    "        output_file = open(f_n,'w')\n",
    "        output_file.write(str(prob_sim_sent))\n",
    "    elif option == 2:\n",
    "        sent_one = input(\"Enter the first sentence : \")\n",
    "        sent_two = input(\"Enter the second sentence two :\")\n",
    "        prob_sim_sent = tokenize(sent_one , sent_two)\n",
    "        print(prob_sim_sent)\n",
    "        #print(\"Similarity between\\n\"+sent_one+\"\\n\"+sent_two+\"\\n\\n is : \",prob_sim_sent)\n",
    "    else:\n",
    "        global max_count\n",
    "        if max_count < 3 : print(\"Wrong Choice Try again\"); max_count+=1 \n",
    "        else: print(\"Wrong choice time exceeded!\");exit()\n",
    "        intro()\n",
    "        \n",
    "if __name__ == \"__main__\":  \n",
    "    print(\"-------------------Sentence Similarity--------------------------\")\n",
    "    intro()\n",
    "    print(\"Want to try once again? if yes press 1 or else 0\")\n",
    "    excited = int(input())\n",
    "    while(excited == 1):\n",
    "        intro()\n",
    "        print(\"Want to try once again?\")\n",
    "        excited = int(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32989370842\n",
      "0.392684764655\n",
      "0.215971016507\n",
      "0.309323112201\n",
      "0.364590693528\n",
      "0.378395628396\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "import math\n",
    "import pandas as pd\n",
    "CONST_PHI = 0.2\n",
    "CONST_BETA = 0.45\n",
    "CONST_ALPHA = 0.2\n",
    "CONST_PHI = 0.2\n",
    "CONST_DELTA = 0.875\n",
    "CONST_ETA = 0.4\n",
    "total_words = 0\n",
    "word_freq_brown = {}\n",
    "\n",
    "def Get_NewCodes():\n",
    "    df = pd.read_csv(\"Same_SIM_Draft.csv\")\n",
    "    sCodes = df['LongDesc'].tolist()\n",
    "    return sCodes; \n",
    "        \n",
    "def Get_MasterCodeDesc():\n",
    "    df = pd.read_csv(\"Same_SIM_Master_Draft.csv\")\n",
    "    sMasterCodeDesc = df['CPTLongDesc'].tolist()\n",
    "    return sMasterCodeDesc;\n",
    "\n",
    "\n",
    "def proper_synset(word_one , word_two):\n",
    "    pair = (None,None)\n",
    "    maximum_similarity = -1\n",
    "    synsets_one = wn.synsets(word_one)\n",
    "    synsets_two = wn.synsets(word_two)\n",
    "    #print(\"first word :\",word_one)\n",
    "    #print(\"second word\",word_two)\n",
    "    #print(synsets_one)\n",
    "    #print(synsets_two)\n",
    "    if(len(synsets_one)!=0 and len(synsets_two)!=0):\n",
    "        for synset_one in synsets_one:\n",
    "            for synset_two in synsets_two:\n",
    "                similarity = wn.path_similarity(synset_one,synset_two)\n",
    "                if(similarity == None):\n",
    "                    sim = -2\n",
    "                elif(similarity > maximum_similarity):\n",
    "                    maximum_similarity = similarity\n",
    "                    pair = synset_one,synset_two\n",
    "    else:\n",
    "        #need to see as for some word there will be no wordset.\n",
    "        #shuld make it as none\n",
    "        pair = (None , None)\n",
    "    return pair\n",
    "def length_between_words(synset_one , synset_two):\n",
    "    length = 100000000\n",
    "    if synset_one is None or synset_two is None:\n",
    "        return 0\n",
    "    elif(synset_one == synset_two):\n",
    "        length = 0\n",
    "    else:\n",
    "        words_synet1 = set([word.name() for word in synset_one.lemmas()])\n",
    "        words_synet2 = set([word.name() for word in synset_two.lemmas()])\n",
    "        if(len(words_synet1) + len(words_synet2) > len(words_synet1.union(words_synet2))):\n",
    "            length = 0\n",
    "        else:\n",
    "            #finding the actual distance\n",
    "            length = synset_one.shortest_path_distance(synset_two)\n",
    "            if(length is None):\n",
    "                return 0\n",
    "    return math.exp( -1 * CONST_ALPHA * length)\n",
    "def depth_common_subsumer(synset_one,synset_two):\n",
    "    height = 100000000\n",
    "    if synset_one is None or synset_two is None:\n",
    "        return 0\n",
    "    elif synset_one == synset_two:\n",
    "        height = max([hypernym[1] for hypernym in synset_one.hypernym_distances()])\n",
    "    else:\n",
    "        #get the hypernym set of both the synset.\n",
    "        hypernym_one = {hypernym_word[0]:hypernym_word[1] for hypernym_word in synset_one.hypernym_distances()}\n",
    "        hypernym_two = {hypernym_word[0]:hypernym_word[1] for hypernym_word in synset_two.hypernym_distances()}\n",
    "        common_subsumer = set(hypernym_one.keys()).intersection(set(hypernym_two.keys()))\n",
    "        if(len(common_subsumer) == 0):\n",
    "            height = 0\n",
    "        else:\n",
    "            height = 0\n",
    "            for cs in common_subsumer:\n",
    "                val = [hypernym_word[1] for hypernym_word in cs.hypernym_distances()]\n",
    "                val = max(val)\n",
    "                if val > height : height = val\n",
    "\n",
    "    #print(height) #works\n",
    "    return (math.exp(CONST_BETA * height) - math.exp(-CONST_BETA * height))/(math.exp(CONST_BETA * height) + math.exp(-CONST_BETA * height))\n",
    "def word_similarity(word1,word2):\n",
    "    #depth_common_subsumer(wn.synset('boy.n.01'),wn.synset('life_form.n.01'))\n",
    "    #print(wn.synset('boy.n.01').lowest_common_hypernym(wn.synset('animal.n.01')))\n",
    "    #print(wn.synset('boy.n.01').lowest_common_hypernym(wn.synset('girl.n.01')))\n",
    "    #word1 = input(\"Enter the first word: \")\n",
    "    #word2 = input(\"Enter the second word: \")\n",
    "    #synset_wordone = wn.synset(word1+\".n.01\")#doesnt work\n",
    "    #synset_wordtwo = wn.synset(word2+\".n.01\")#doesnt work\n",
    "    synset_wordone,synset_wordtwo = proper_synset(word1,word2) # cant just add +\".n.01\" to words to convert them to a synset.\n",
    "    #Need to execute the above as we cant know whether a 'noun' for of the word exists or not.\n",
    "    return length_between_words(synset_wordone,synset_wordtwo) * depth_common_subsumer(synset_wordone,synset_wordtwo)\n",
    "\n",
    "def I(search_word):\n",
    "    global total_words\n",
    "    if(total_words == 0):\n",
    "        for sent in brown.sents():\n",
    "            for word in sent:\n",
    "                word = word.lower()\n",
    "                if word not in word_freq_brown:\n",
    "                    word_freq_brown[word] = 0\n",
    "                word_freq_brown[word] +=1\n",
    "                total_words+=1\n",
    "    count = 0 if search_word not in word_freq_brown else word_freq_brown[search_word]\n",
    "    ret = 1.0 - (math.log(count+1)/math.log(total_words+1))\n",
    "    return ret\n",
    "def most_similar_word(word,sentence):\n",
    "    most_similarity = 0\n",
    "    most_similar_word = ''\n",
    "    for w in sentence:\n",
    "        #compute the word similarity using the already defined function\n",
    "        sim  =  word_similarity(w,word)\n",
    "        if sim > most_similarity:\n",
    "            most_similarity = sim\n",
    "            most_similar_word = w\n",
    "    if most_similarity <= CONST_PHI:\n",
    "        most_similarity = 0\n",
    "    return most_similar_word,most_similarity \n",
    "\n",
    "def gen_sem_vec(sentence , joint_word_set):\n",
    "    semantic_vector = np.zeros(len(joint_word_set))\n",
    "    #print(semantic_vector)\n",
    "    i = 0\n",
    "    #print(\"This is sentence :\",sentence)\n",
    "    #print(\"This is joint word set:\",joint_word_set)\n",
    "    for joint_word in joint_word_set:\n",
    "        sim_word = joint_word # to measure the \n",
    "        beta_sim_measure = 1\n",
    "        if (joint_word in sentence):\n",
    "            pass\n",
    "        else:\n",
    "            sim_word,beta_sim_measure = most_similar_word(joint_word,sentence) # gets the most similar word in that sentence.\n",
    "            beta_sim_measure = 0 if beta_sim_measure <= CONST_PHI else beta_sim_measure\n",
    "        sim_measure = beta_sim_measure * I(joint_word) * I(sim_word)\n",
    "        #sim_measure = beta_sim_measure ##Without information content which is got from the corpus.\n",
    "        semantic_vector[i] = sim_measure\n",
    "        i+=1\n",
    "    return semantic_vector\n",
    "def sent_sim(sent_set_one, sent_set_two , joint_word_set):\n",
    "    #sent_set_one = set(filter(lambda x : not (x == '.' or x == '?') , word_tokenize(sentence_one)))\n",
    "    #sent_set_two = set(filter(lambda x : not (x == '.' or x == '?') , word_tokenize(sentence_two)))\n",
    "    #print(sent_set_one)    \n",
    "    #print(sent_set_two)\n",
    "    #print(list(sent_set_one.union(sent_set_two)))\n",
    "    #joint_word_set = list(sent_set_one.union(sent_set_two))\n",
    "    #print(joint_word_set)\n",
    "    #sent_set_one = list(sent_set_one)\n",
    "    #sent_set_two = list(sent_set_two)\n",
    "    sem_vec_one = gen_sem_vec(sent_set_one,joint_word_set)\n",
    "    sem_vec_two = gen_sem_vec(sent_set_two,joint_word_set)\n",
    "    #multiply the two vectors..\n",
    "    #print(sem_vec_one)\n",
    "    #print(sem_vec_two)\n",
    "    return np.dot(sem_vec_one,sem_vec_two.T) / (np.linalg.norm(sem_vec_one) * np.linalg.norm(sem_vec_two))\n",
    "def word_order_similarity(sentence_one , sentence_two):\n",
    "    #print(\"Sentence one :\",sentence_one)\n",
    "    token_one  = word_tokenize(sentence_one)\n",
    "    #print(\"Sentence two : \",sentence_two)\n",
    "    token_two = word_tokenize(sentence_two)\n",
    "    joint_word_set = list(set(token_one).union(set(token_two)))\n",
    "    r1 = np.zeros(len(joint_word_set))\n",
    "    r2 = np.zeros(len(joint_word_set))\n",
    "    #filling for the first one\n",
    "    en_joint_one = {x[1]:x[0] for x in enumerate(token_one)}\n",
    "    en_joint_two = {x[1]:x[0] for x in enumerate(token_two)}\n",
    "    set_token_one = set(token_one)\n",
    "    set_token_two = set(token_two)\n",
    "    i = 0\n",
    "    #print(en_joint)\n",
    "    for word in joint_word_set:\n",
    "        if word in set_token_one:\n",
    "            r1[i] = en_joint_one[word]#so wrong.\n",
    "        else:\n",
    "            #get best word and check if its greater then a preset threshold\n",
    "            sim_word , sim = most_similar_word(word , list(set_token_one))\n",
    "            if sim > CONST_ETA : \n",
    "                r1[i] = en_joint_one[sim_word]\n",
    "            else:\n",
    "                r1[i] = 0\n",
    "        i+=1\n",
    "    j = 0\n",
    "    for word in joint_word_set:\n",
    "        if word in set_token_two:\n",
    "            r2[j] = en_joint_two[word]\n",
    "        else:\n",
    "            #get best word and check if its greater then a preset threshold\n",
    "            sim_word , sim = most_similar_word(word , list(set_token_two))\n",
    "            if sim > CONST_ETA : \n",
    "                r2[j] = en_joint_two[sim_word]\n",
    "            else:\n",
    "                r2[j] = 0\n",
    "        j+=1\n",
    "    return 1.0 - (np.linalg.norm(r1 - r2) / np.linalg.norm(r1 + r2))\n",
    "def main(sentence_one,sentence_two):\n",
    "    sent_set_one = set(filter(lambda x : not (x == '.' or x == '?') , word_tokenize(sentence_one)))\n",
    "    sent_set_two = set(filter(lambda x : not (x == '.' or x == '?') , word_tokenize(sentence_two)))\n",
    "    joint_word_set = list(sent_set_one.union(sent_set_two))\n",
    "    #Need to get the dictionary to have the corresponding indexes of the joint_word_set. \n",
    "    sentence_similarity = (CONST_DELTA * sent_sim(sent_set_one,sent_set_two,list(joint_word_set))) + ((1.0 - CONST_DELTA) * word_order_similarity(sentence_one,sentence_two))\n",
    "    return sentence_similarity\n",
    "#sentence_one = \"I play hockey\"\n",
    "#sentence_two = \"who are you?\"\n",
    "#print(main(sentence_one,sentence_two))\\\n",
    "def file_sem(f):\n",
    "    contents = open(f).read().strip()\n",
    "    ind_sentences = sent_tokenize(contents)\n",
    "    #print(ind_sentences)\n",
    "    no_of_sentences = len(ind_sentences)\n",
    "    sent_sim_matr = np.zeros((no_of_sentences,no_of_sentences))\n",
    "    i = 0\n",
    "    print(ind_sentences)\n",
    "    while(i < no_of_sentences):\n",
    "        j = i\n",
    "        while(j < no_of_sentences):\n",
    "            sent_sim_matr[i][j] = main(ind_sentences[i],ind_sentences[j])\n",
    "            sent_sim_matr[j][i] = sent_sim_matr[i][j]\n",
    "            j+=1\n",
    "        i+=1\n",
    "    return sent_sim_matr\n",
    "\n",
    "def intro():\n",
    "    sent_one = Get_NewCodes()\n",
    "    sent_two = Get_MasterCodeDesc()\n",
    "    for iNewLongDesc in sent_one:  \n",
    "            for iMasterDesc in sent_two:\n",
    "                    prob_sim_sent = main(iNewLongDesc , iMasterDesc)\n",
    "                    print(prob_sim_sent)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        intro()\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method ivoked\n",
      "opened file\n",
      "read 0 reviews\n",
      "read 10 reviews\n",
      "read 20 reviews\n",
      "read 30 reviews\n",
      "read 40 reviews\n",
      "read 50 reviews\n",
      "read 60 reviews\n",
      "read 70 reviews\n",
      "read 80 reviews\n",
      "read 90 reviews\n",
      "read 100 reviews\n",
      "read 110 reviews\n",
      "read 120 reviews\n",
      "read 130 reviews\n",
      "read 140 reviews\n",
      "read 150 reviews\n",
      "read 160 reviews\n",
      "read 170 reviews\n",
      "read 180 reviews\n",
      "read 190 reviews\n",
      "read 200 reviews\n",
      "read 210 reviews\n",
      "read 220 reviews\n",
      "read 230 reviews\n",
      "read 240 reviews\n",
      "read 250 reviews\n",
      "read 260 reviews\n",
      "read 270 reviews\n",
      "read 280 reviews\n",
      "read 290 reviews\n",
      "read 300 reviews\n",
      "read 310 reviews\n",
      "read 320 reviews\n",
      "read 330 reviews\n",
      "read 340 reviews\n",
      "read 350 reviews\n",
      "read 360 reviews\n",
      "read 370 reviews\n",
      "read 380 reviews\n",
      "read 390 reviews\n",
      "read 400 reviews\n",
      "read 410 reviews\n",
      "read 420 reviews\n",
      "read 430 reviews\n",
      "read 440 reviews\n",
      "read 450 reviews\n",
      "read 460 reviews\n",
      "read 470 reviews\n",
      "read 480 reviews\n",
      "read 490 reviews\n",
      "read 500 reviews\n",
      "read 510 reviews\n",
      "read 520 reviews\n",
      "read 530 reviews\n",
      "read 540 reviews\n",
      "read 550 reviews\n",
      "read 560 reviews\n",
      "read 570 reviews\n",
      "read 580 reviews\n",
      "read 590 reviews\n",
      "read 600 reviews\n",
      "read 610 reviews\n",
      "read 620 reviews\n",
      "read 630 reviews\n",
      "read 640 reviews\n",
      "read 650 reviews\n",
      "read 660 reviews\n",
      "read 670 reviews\n",
      "read 680 reviews\n",
      "read 690 reviews\n",
      "read 700 reviews\n",
      "read 710 reviews\n",
      "read 720 reviews\n",
      "read 730 reviews\n",
      "read 740 reviews\n",
      "read 750 reviews\n",
      "read 760 reviews\n",
      "read 770 reviews\n",
      "read 780 reviews\n",
      "read 790 reviews\n",
      "read 800 reviews\n",
      "read 810 reviews\n",
      "read 820 reviews\n",
      "read 830 reviews\n",
      "read 840 reviews\n",
      "read 850 reviews\n",
      "read 860 reviews\n",
      "read 870 reviews\n",
      "read 880 reviews\n",
      "read 890 reviews\n",
      "read 900 reviews\n",
      "read 910 reviews\n",
      "read 920 reviews\n",
      "read 930 reviews\n",
      "read 940 reviews\n",
      "read 950 reviews\n",
      "read 960 reviews\n",
      "read 970 reviews\n",
      "read 980 reviews\n",
      "read 990 reviews\n",
      "read 1000 reviews\n",
      "read 1010 reviews\n",
      "read 1020 reviews\n",
      "read 1030 reviews\n",
      "read 1040 reviews\n",
      "read 1050 reviews\n",
      "read 1060 reviews\n",
      "read 1070 reviews\n",
      "read 1080 reviews\n",
      "read 1090 reviews\n",
      "read 1100 reviews\n",
      "read 1110 reviews\n",
      "read 1120 reviews\n",
      "read 1130 reviews\n",
      "read 1140 reviews\n",
      "read 1150 reviews\n",
      "read 1160 reviews\n",
      "read 1170 reviews\n",
      "read 1180 reviews\n",
      "read 1190 reviews\n",
      "read 1200 reviews\n",
      "read 1210 reviews\n",
      "read 1220 reviews\n",
      "read 1230 reviews\n",
      "read 1240 reviews\n",
      "read 1250 reviews\n",
      "read 1260 reviews\n",
      "read 1270 reviews\n",
      "read 1280 reviews\n",
      "read 1290 reviews\n",
      "read 1300 reviews\n",
      "read 1310 reviews\n",
      "read 1320 reviews\n",
      "read 1330 reviews\n",
      "read 1340 reviews\n",
      "read 1350 reviews\n",
      "read 1360 reviews\n",
      "read 1370 reviews\n",
      "read 1380 reviews\n",
      "read 1390 reviews\n",
      "read 1400 reviews\n",
      "read 1410 reviews\n",
      "read 1420 reviews\n",
      "read 1430 reviews\n",
      "read 1440 reviews\n",
      "read 1450 reviews\n",
      "read 1460 reviews\n",
      "read 1470 reviews\n",
      "read 1480 reviews\n",
      "read 1490 reviews\n",
      "read 1500 reviews\n",
      "read 1510 reviews\n",
      "read 1520 reviews\n",
      "read 1530 reviews\n",
      "read 1540 reviews\n",
      "read 1550 reviews\n",
      "read 1560 reviews\n",
      "read 1570 reviews\n",
      "read 1580 reviews\n",
      "read 1590 reviews\n",
      "read 1600 reviews\n",
      "read 1610 reviews\n",
      "read 1620 reviews\n",
      "read 1630 reviews\n",
      "read 1640 reviews\n",
      "read 1650 reviews\n",
      "read 1660 reviews\n",
      "read 1670 reviews\n",
      "read 1680 reviews\n",
      "read 1690 reviews\n",
      "read 1700 reviews\n",
      "read 1710 reviews\n",
      "read 1720 reviews\n",
      "read 1730 reviews\n",
      "read 1740 reviews\n",
      "read 1750 reviews\n",
      "read 1760 reviews\n",
      "read 1770 reviews\n",
      "read 1780 reviews\n",
      "read 1790 reviews\n",
      "read 1800 reviews\n",
      "read 1810 reviews\n",
      "read 1820 reviews\n",
      "read 1830 reviews\n",
      "read 1840 reviews\n",
      "read 1850 reviews\n",
      "read 1860 reviews\n",
      "read 1870 reviews\n",
      "read 1880 reviews\n",
      "read 1890 reviews\n",
      "read 1900 reviews\n",
      "read 1910 reviews\n",
      "read 1920 reviews\n",
      "read 1930 reviews\n",
      "read 1940 reviews\n",
      "read 1950 reviews\n",
      "read 1960 reviews\n",
      "read 1970 reviews\n",
      "read 1980 reviews\n",
      "read 1990 reviews\n",
      "read 2000 reviews\n",
      "read 2010 reviews\n",
      "read 2020 reviews\n",
      "read 2030 reviews\n",
      "read 2040 reviews\n",
      "read 2050 reviews\n",
      "read 2060 reviews\n",
      "read 2070 reviews\n",
      "read 2080 reviews\n",
      "read 2090 reviews\n",
      "read 2100 reviews\n",
      "read 2110 reviews\n",
      "read 2120 reviews\n",
      "read 2130 reviews\n",
      "read 2140 reviews\n",
      "read 2150 reviews\n",
      "read 2160 reviews\n",
      "read 2170 reviews\n",
      "read 2180 reviews\n",
      "read 2190 reviews\n",
      "read 2200 reviews\n",
      "read 2210 reviews\n",
      "read 2220 reviews\n",
      "read 2230 reviews\n",
      "read 2240 reviews\n",
      "read 2250 reviews\n",
      "read 2260 reviews\n",
      "read 2270 reviews\n",
      "read 2280 reviews\n",
      "read 2290 reviews\n",
      "read 2300 reviews\n",
      "read 2310 reviews\n",
      "read 2320 reviews\n",
      "read 2330 reviews\n",
      "read 2340 reviews\n",
      "read 2350 reviews\n",
      "read 2360 reviews\n",
      "read 2370 reviews\n",
      "read 2380 reviews\n",
      "read 2390 reviews\n",
      "read 2400 reviews\n",
      "read 2410 reviews\n",
      "read 2420 reviews\n",
      "read 2430 reviews\n",
      "read 2440 reviews\n",
      "read 2450 reviews\n",
      "read 2460 reviews\n",
      "read 2470 reviews\n",
      "read 2480 reviews\n",
      "read 2490 reviews\n",
      "read 2500 reviews\n",
      "read 2510 reviews\n",
      "read 2520 reviews\n",
      "read 2530 reviews\n",
      "read 2540 reviews\n",
      "read 2550 reviews\n",
      "read 2560 reviews\n",
      "read 2570 reviews\n",
      "read 2580 reviews\n",
      "read 2590 reviews\n",
      "read 2600 reviews\n",
      "read 2610 reviews\n",
      "read 2620 reviews\n",
      "read 2630 reviews\n",
      "read 2640 reviews\n",
      "read 2650 reviews\n",
      "read 2660 reviews\n",
      "read 2670 reviews\n",
      "read 2680 reviews\n",
      "read 2690 reviews\n",
      "read 2700 reviews\n",
      "read 2710 reviews\n",
      "read 2720 reviews\n",
      "read 2730 reviews\n",
      "read 2740 reviews\n",
      "read 2750 reviews\n",
      "read 2760 reviews\n",
      "read 2770 reviews\n",
      "read 2780 reviews\n",
      "read 2790 reviews\n",
      "read 2800 reviews\n",
      "read 2810 reviews\n",
      "read 2820 reviews\n",
      "read 2830 reviews\n",
      "read 2840 reviews\n",
      "read 2850 reviews\n",
      "read 2860 reviews\n",
      "read 2870 reviews\n",
      "read 2880 reviews\n",
      "read 2890 reviews\n",
      "read 2900 reviews\n",
      "read 2910 reviews\n",
      "read 2920 reviews\n",
      "read 2930 reviews\n",
      "read 2940 reviews\n",
      "read 2950 reviews\n",
      "read 2960 reviews\n",
      "read 2970 reviews\n",
      "read 2980 reviews\n",
      "read 2990 reviews\n",
      "read 3000 reviews\n",
      "read 3010 reviews\n",
      "read 3020 reviews\n",
      "read 3030 reviews\n",
      "read 3040 reviews\n",
      "read 3050 reviews\n",
      "read 3060 reviews\n",
      "read 3070 reviews\n",
      "read 3080 reviews\n",
      "read 3090 reviews\n",
      "read 3100 reviews\n",
      "read 3110 reviews\n",
      "read 3120 reviews\n",
      "read 3130 reviews\n",
      "read 3140 reviews\n",
      "read 3150 reviews\n",
      "read 3160 reviews\n",
      "read 3170 reviews\n",
      "read 3180 reviews\n",
      "read 3190 reviews\n",
      "read 3200 reviews\n",
      "read 3210 reviews\n",
      "read 3220 reviews\n",
      "read 3230 reviews\n",
      "read 3240 reviews\n",
      "read 3250 reviews\n",
      "read 3260 reviews\n",
      "read 3270 reviews\n",
      "read 3280 reviews\n",
      "read 3290 reviews\n",
      "read 3300 reviews\n",
      "read 3310 reviews\n",
      "read 3320 reviews\n",
      "read 3330 reviews\n",
      "read 3340 reviews\n",
      "read 3350 reviews\n",
      "read 3360 reviews\n",
      "read 3370 reviews\n",
      "read 3380 reviews\n",
      "read 3390 reviews\n",
      "read 3400 reviews\n",
      "read 3410 reviews\n",
      "read 3420 reviews\n",
      "read 3430 reviews\n",
      "read 3440 reviews\n",
      "read 3450 reviews\n",
      "read 3460 reviews\n",
      "read 3470 reviews\n",
      "read 3480 reviews\n",
      "read 3490 reviews\n",
      "read 3500 reviews\n",
      "read 3510 reviews\n",
      "read 3520 reviews\n",
      "read 3530 reviews\n",
      "read 3540 reviews\n",
      "read 3550 reviews\n",
      "read 3560 reviews\n",
      "read 3570 reviews\n",
      "read 3580 reviews\n",
      "read 3590 reviews\n",
      "read 3600 reviews\n",
      "read 3610 reviews\n",
      "read 3620 reviews\n",
      "read 3630 reviews\n",
      "read 3640 reviews\n",
      "read 3650 reviews\n",
      "read 3660 reviews\n",
      "read 3670 reviews\n",
      "read 3680 reviews\n",
      "read 3690 reviews\n",
      "read 3700 reviews\n",
      "read 3710 reviews\n",
      "read 3720 reviews\n",
      "read 3730 reviews\n",
      "read 3740 reviews\n",
      "read 3750 reviews\n",
      "read 3760 reviews\n",
      "read 3770 reviews\n",
      "read 3780 reviews\n",
      "read 3790 reviews\n",
      "read 3800 reviews\n",
      "read 3810 reviews\n",
      "read 3820 reviews\n",
      "read 3830 reviews\n",
      "read 3840 reviews\n",
      "read 3850 reviews\n",
      "read 3860 reviews\n",
      "read 3870 reviews\n",
      "read 3880 reviews\n",
      "read 3890 reviews\n",
      "read 3900 reviews\n",
      "read 3910 reviews\n",
      "read 3920 reviews\n",
      "read 3930 reviews\n",
      "read 3940 reviews\n",
      "read 3950 reviews\n",
      "read 3960 reviews\n",
      "read 3970 reviews\n",
      "read 3980 reviews\n",
      "read 3990 reviews\n",
      "read 4000 reviews\n",
      "read 4010 reviews\n",
      "read 4020 reviews\n",
      "read 4030 reviews\n",
      "read 4040 reviews\n",
      "read 4050 reviews\n",
      "read 4060 reviews\n",
      "read 4070 reviews\n",
      "read 4080 reviews\n",
      "read 4090 reviews\n",
      "read 4100 reviews\n",
      "read 4110 reviews\n",
      "read 4120 reviews\n",
      "read 4130 reviews\n",
      "read 4140 reviews\n",
      "read 4150 reviews\n",
      "read 4160 reviews\n",
      "read 4170 reviews\n",
      "read 4180 reviews\n",
      "read 4190 reviews\n",
      "read 4200 reviews\n",
      "read 4210 reviews\n",
      "read 4220 reviews\n",
      "read 4230 reviews\n",
      "read 4240 reviews\n",
      "read 4250 reviews\n",
      "read 4260 reviews\n",
      "read 4270 reviews\n",
      "read 4280 reviews\n",
      "read 4290 reviews\n",
      "read 4300 reviews\n",
      "read 4310 reviews\n",
      "read 4320 reviews\n",
      "read 4330 reviews\n",
      "read 4340 reviews\n",
      "read 4350 reviews\n",
      "read 4360 reviews\n",
      "read 4370 reviews\n",
      "read 4380 reviews\n",
      "read 4390 reviews\n",
      "read 4400 reviews\n",
      "read 4410 reviews\n",
      "read 4420 reviews\n",
      "read 4430 reviews\n",
      "read 4440 reviews\n",
      "read 4450 reviews\n",
      "read 4460 reviews\n",
      "read 4470 reviews\n",
      "read 4480 reviews\n",
      "read 4490 reviews\n",
      "read 4500 reviews\n",
      "read 4510 reviews\n",
      "read 4520 reviews\n",
      "read 4530 reviews\n",
      "read 4540 reviews\n",
      "read 4550 reviews\n",
      "read 4560 reviews\n",
      "read 4570 reviews\n",
      "read 4580 reviews\n",
      "read 4590 reviews\n",
      "read 4600 reviews\n",
      "read 4610 reviews\n",
      "read 4620 reviews\n",
      "read 4630 reviews\n",
      "read 4640 reviews\n",
      "read 4650 reviews\n",
      "read 4660 reviews\n",
      "read 4670 reviews\n",
      "read 4680 reviews\n",
      "read 4690 reviews\n",
      "read 4700 reviews\n",
      "read 4710 reviews\n",
      "read 4720 reviews\n",
      "read 4730 reviews\n",
      "read 4740 reviews\n",
      "read 4750 reviews\n",
      "read 4760 reviews\n",
      "read 4770 reviews\n",
      "read 4780 reviews\n",
      "read 4790 reviews\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 4800 reviews\n",
      "read 4810 reviews\n",
      "read 4820 reviews\n",
      "read 4830 reviews\n",
      "read 4840 reviews\n",
      "read 4850 reviews\n",
      "read 4860 reviews\n",
      "read 4870 reviews\n",
      "read 4880 reviews\n",
      "read 4890 reviews\n",
      "read 4900 reviews\n",
      "read 4910 reviews\n",
      "read 4920 reviews\n",
      "read 4930 reviews\n",
      "read 4940 reviews\n",
      "read 4950 reviews\n",
      "read 4960 reviews\n",
      "read 4970 reviews\n",
      "read 4980 reviews\n",
      "read 4990 reviews\n",
      "read 5000 reviews\n",
      "read 5010 reviews\n",
      "read 5020 reviews\n",
      "read 5030 reviews\n",
      "read 5040 reviews\n",
      "read 5050 reviews\n",
      "read 5060 reviews\n",
      "read 5070 reviews\n",
      "read 5080 reviews\n",
      "read 5090 reviews\n",
      "read 5100 reviews\n",
      "read 5110 reviews\n",
      "read 5120 reviews\n",
      "read 5130 reviews\n",
      "read 5140 reviews\n",
      "read 5150 reviews\n",
      "read 5160 reviews\n",
      "read 5170 reviews\n",
      "read 5180 reviews\n",
      "read 5190 reviews\n",
      "read 5200 reviews\n",
      "read 5210 reviews\n",
      "read 5220 reviews\n",
      "read 5230 reviews\n",
      "read 5240 reviews\n",
      "read 5250 reviews\n",
      "read 5260 reviews\n",
      "read 5270 reviews\n",
      "read 5280 reviews\n",
      "read 5290 reviews\n",
      "read 5300 reviews\n",
      "read 5310 reviews\n",
      "read 5320 reviews\n",
      "read 5330 reviews\n",
      "read 5340 reviews\n",
      "read 5350 reviews\n",
      "read 5360 reviews\n",
      "read 5370 reviews\n",
      "read 5380 reviews\n",
      "read 5390 reviews\n",
      "read 5400 reviews\n",
      "read 5410 reviews\n",
      "read 5420 reviews\n",
      "read 5430 reviews\n",
      "read 5440 reviews\n",
      "read 5450 reviews\n",
      "read 5460 reviews\n",
      "read 5470 reviews\n",
      "read 5480 reviews\n",
      "read 5490 reviews\n",
      "read 5500 reviews\n",
      "read 5510 reviews\n",
      "read 5520 reviews\n",
      "read 5530 reviews\n",
      "read 5540 reviews\n",
      "read 5550 reviews\n",
      "read 5560 reviews\n",
      "read 5570 reviews\n",
      "read 5580 reviews\n",
      "read 5590 reviews\n",
      "read 5600 reviews\n",
      "read 5610 reviews\n",
      "read 5620 reviews\n",
      "read 5630 reviews\n",
      "read 5640 reviews\n",
      "read 5650 reviews\n",
      "read 5660 reviews\n",
      "read 5670 reviews\n",
      "read 5680 reviews\n",
      "read 5690 reviews\n",
      "read 5700 reviews\n",
      "read 5710 reviews\n",
      "read 5720 reviews\n",
      "read 5730 reviews\n",
      "read 5740 reviews\n",
      "read 5750 reviews\n",
      "read 5760 reviews\n",
      "read 5770 reviews\n",
      "read 5780 reviews\n",
      "read 5790 reviews\n",
      "read 5800 reviews\n",
      "read 5810 reviews\n",
      "read 5820 reviews\n",
      "read 5830 reviews\n",
      "read 5840 reviews\n",
      "read 5850 reviews\n",
      "read 5860 reviews\n",
      "read 5870 reviews\n",
      "read 5880 reviews\n",
      "read 5890 reviews\n",
      "read 5900 reviews\n",
      "read 5910 reviews\n",
      "read 5920 reviews\n",
      "read 5930 reviews\n",
      "read 5940 reviews\n",
      "read 5950 reviews\n",
      "read 5960 reviews\n",
      "read 5970 reviews\n",
      "read 5980 reviews\n",
      "read 5990 reviews\n",
      "read 6000 reviews\n",
      "read 6010 reviews\n",
      "read 6020 reviews\n",
      "read 6030 reviews\n",
      "read 6040 reviews\n",
      "read 6050 reviews\n",
      "read 6060 reviews\n",
      "read 6070 reviews\n",
      "read 6080 reviews\n",
      "read 6090 reviews\n",
      "read 6100 reviews\n",
      "read 6110 reviews\n",
      "read 6120 reviews\n",
      "read 6130 reviews\n",
      "read 6140 reviews\n",
      "read 6150 reviews\n",
      "read 6160 reviews\n",
      "read 6170 reviews\n",
      "read 6180 reviews\n",
      "read 6190 reviews\n",
      "read 6200 reviews\n",
      "read 6210 reviews\n",
      "read 6220 reviews\n",
      "read 6230 reviews\n",
      "read 6240 reviews\n",
      "read 6250 reviews\n",
      "read 6260 reviews\n",
      "read 6270 reviews\n",
      "read 6280 reviews\n",
      "read 6290 reviews\n",
      "read 6300 reviews\n",
      "read 6310 reviews\n",
      "read 6320 reviews\n",
      "read 6330 reviews\n",
      "read 6340 reviews\n",
      "read 6350 reviews\n",
      "read 6360 reviews\n",
      "read 6370 reviews\n",
      "read 6380 reviews\n",
      "read 6390 reviews\n",
      "read 6400 reviews\n",
      "read 6410 reviews\n",
      "read 6420 reviews\n",
      "read 6430 reviews\n",
      "read 6440 reviews\n",
      "read 6450 reviews\n",
      "read 6460 reviews\n",
      "read 6470 reviews\n",
      "read 6480 reviews\n",
      "read 6490 reviews\n",
      "read 6500 reviews\n",
      "read 6510 reviews\n",
      "read 6520 reviews\n",
      "read 6530 reviews\n",
      "read 6540 reviews\n",
      "read 6550 reviews\n",
      "read 6560 reviews\n",
      "read 6570 reviews\n",
      "read 6580 reviews\n",
      "read 6590 reviews\n",
      "read 6600 reviews\n",
      "read 6610 reviews\n",
      "read 6620 reviews\n",
      "read 6630 reviews\n",
      "read 6640 reviews\n",
      "read 6650 reviews\n",
      "read 6660 reviews\n",
      "read 6670 reviews\n",
      "read 6680 reviews\n",
      "read 6690 reviews\n",
      "read 6700 reviews\n",
      "read 6710 reviews\n",
      "read 6720 reviews\n",
      "read 6730 reviews\n",
      "read 6740 reviews\n",
      "read 6750 reviews\n",
      "read 6760 reviews\n",
      "read 6770 reviews\n",
      "read 6780 reviews\n",
      "read 6790 reviews\n",
      "read 6800 reviews\n",
      "read 6810 reviews\n",
      "read 6820 reviews\n",
      "read 6830 reviews\n",
      "read 6840 reviews\n",
      "read 6850 reviews\n",
      "read 6860 reviews\n",
      "read 6870 reviews\n",
      "read 6880 reviews\n",
      "read 6890 reviews\n",
      "read 6900 reviews\n",
      "read 6910 reviews\n",
      "read 6920 reviews\n",
      "read 6930 reviews\n",
      "read 6940 reviews\n",
      "read 6950 reviews\n",
      "read 6960 reviews\n",
      "read 6970 reviews\n",
      "read 6980 reviews\n",
      "read 6990 reviews\n",
      "read 7000 reviews\n",
      "read 7010 reviews\n",
      "read 7020 reviews\n",
      "read 7030 reviews\n",
      "read 7040 reviews\n",
      "read 7050 reviews\n",
      "read 7060 reviews\n",
      "read 7070 reviews\n",
      "read 7080 reviews\n",
      "read 7090 reviews\n",
      "read 7100 reviews\n",
      "read 7110 reviews\n",
      "read 7120 reviews\n",
      "read 7130 reviews\n",
      "read 7140 reviews\n",
      "read 7150 reviews\n",
      "read 7160 reviews\n",
      "read 7170 reviews\n",
      "read 7180 reviews\n",
      "read 7190 reviews\n",
      "read 7200 reviews\n",
      "read 7210 reviews\n",
      "read 7220 reviews\n",
      "read 7230 reviews\n",
      "read 7240 reviews\n",
      "read 7250 reviews\n",
      "read 7260 reviews\n",
      "read 7270 reviews\n",
      "read 7280 reviews\n",
      "read 7290 reviews\n",
      "read 7300 reviews\n",
      "read 7310 reviews\n",
      "read 7320 reviews\n",
      "read 7330 reviews\n",
      "read 7340 reviews\n",
      "read 7350 reviews\n",
      "read 7360 reviews\n",
      "read 7370 reviews\n",
      "read 7380 reviews\n",
      "read 7390 reviews\n",
      "read 7400 reviews\n",
      "read 7410 reviews\n",
      "read 7420 reviews\n",
      "read 7430 reviews\n",
      "read 7440 reviews\n",
      "read 7450 reviews\n",
      "read 7460 reviews\n",
      "read 7470 reviews\n",
      "read 7480 reviews\n",
      "read 7490 reviews\n",
      "read 7500 reviews\n",
      "read 7510 reviews\n",
      "read 7520 reviews\n",
      "read 7530 reviews\n",
      "read 7540 reviews\n",
      "read 7550 reviews\n",
      "read 7560 reviews\n",
      "read 7570 reviews\n",
      "read 7580 reviews\n",
      "read 7590 reviews\n",
      "read 7600 reviews\n",
      "read 7610 reviews\n",
      "read 7620 reviews\n",
      "read 7630 reviews\n",
      "read 7640 reviews\n",
      "read 7650 reviews\n",
      "read 7660 reviews\n",
      "read 7670 reviews\n",
      "read 7680 reviews\n",
      "read 7690 reviews\n",
      "read 7700 reviews\n",
      "read 7710 reviews\n",
      "read 7720 reviews\n",
      "read 7730 reviews\n",
      "read 7740 reviews\n",
      "read 7750 reviews\n",
      "read 7760 reviews\n",
      "read 7770 reviews\n",
      "read 7780 reviews\n",
      "read 7790 reviews\n",
      "read 7800 reviews\n",
      "read 7810 reviews\n",
      "read 7820 reviews\n",
      "read 7830 reviews\n",
      "read 7840 reviews\n",
      "read 7850 reviews\n",
      "read 7860 reviews\n",
      "read 7870 reviews\n",
      "read 7880 reviews\n",
      "read 7890 reviews\n",
      "read 7900 reviews\n",
      "read 7910 reviews\n",
      "read 7920 reviews\n",
      "read 7930 reviews\n",
      "read 7940 reviews\n",
      "read 7950 reviews\n",
      "read 7960 reviews\n",
      "read 7970 reviews\n",
      "read 7980 reviews\n",
      "read 7990 reviews\n",
      "read 8000 reviews\n",
      "read 8010 reviews\n",
      "read 8020 reviews\n",
      "read 8030 reviews\n",
      "read 8040 reviews\n",
      "read 8050 reviews\n",
      "read 8060 reviews\n",
      "read 8070 reviews\n",
      "read 8080 reviews\n",
      "read 8090 reviews\n",
      "read 8100 reviews\n",
      "read 8110 reviews\n",
      "read 8120 reviews\n",
      "read 8130 reviews\n",
      "read 8140 reviews\n",
      "read 8150 reviews\n",
      "read 8160 reviews\n",
      "read 8170 reviews\n",
      "read 8180 reviews\n",
      "read 8190 reviews\n",
      "read 8200 reviews\n",
      "read 8210 reviews\n",
      "read 8220 reviews\n",
      "read 8230 reviews\n",
      "read 8240 reviews\n",
      "read 8250 reviews\n",
      "read 8260 reviews\n",
      "read 8270 reviews\n",
      "read 8280 reviews\n",
      "read 8290 reviews\n",
      "read 8300 reviews\n",
      "read 8310 reviews\n",
      "read 8320 reviews\n",
      "read 8330 reviews\n",
      "read 8340 reviews\n",
      "read 8350 reviews\n",
      "read 8360 reviews\n",
      "read 8370 reviews\n",
      "read 8380 reviews\n",
      "read 8390 reviews\n",
      "read 8400 reviews\n",
      "read 8410 reviews\n",
      "read 8420 reviews\n",
      "read 8430 reviews\n",
      "read 8440 reviews\n",
      "read 8450 reviews\n",
      "read 8460 reviews\n",
      "read 8470 reviews\n",
      "read 8480 reviews\n",
      "read 8490 reviews\n",
      "read 8500 reviews\n",
      "read 8510 reviews\n",
      "read 8520 reviews\n",
      "read 8530 reviews\n",
      "read 8540 reviews\n",
      "read 8550 reviews\n",
      "read 8560 reviews\n",
      "read 8570 reviews\n",
      "read 8580 reviews\n",
      "read 8590 reviews\n",
      "read 8600 reviews\n",
      "read 8610 reviews\n",
      "read 8620 reviews\n",
      "read 8630 reviews\n",
      "read 8640 reviews\n",
      "read 8650 reviews\n",
      "read 8660 reviews\n",
      "read 8670 reviews\n",
      "read 8680 reviews\n",
      "read 8690 reviews\n",
      "read 8700 reviews\n",
      "read 8710 reviews\n",
      "read 8720 reviews\n",
      "read 8730 reviews\n",
      "read 8740 reviews\n",
      "read 8750 reviews\n",
      "read 8760 reviews\n",
      "read 8770 reviews\n",
      "read 8780 reviews\n",
      "read 8790 reviews\n",
      "read 8800 reviews\n",
      "read 8810 reviews\n",
      "read 8820 reviews\n",
      "read 8830 reviews\n",
      "read 8840 reviews\n",
      "read 8850 reviews\n",
      "read 8860 reviews\n",
      "read 8870 reviews\n",
      "read 8880 reviews\n",
      "read 8890 reviews\n",
      "read 8900 reviews\n",
      "read 8910 reviews\n",
      "read 8920 reviews\n",
      "read 8930 reviews\n",
      "read 8940 reviews\n",
      "read 8950 reviews\n",
      "read 8960 reviews\n",
      "read 8970 reviews\n",
      "read 8980 reviews\n",
      "read 8990 reviews\n",
      "read 9000 reviews\n",
      "read 9010 reviews\n",
      "read 9020 reviews\n",
      "read 9030 reviews\n",
      "read 9040 reviews\n",
      "read 9050 reviews\n",
      "read 9060 reviews\n",
      "read 9070 reviews\n",
      "read 9080 reviews\n",
      "read 9090 reviews\n",
      "read 9100 reviews\n",
      "read 9110 reviews\n",
      "read 9120 reviews\n",
      "read 9130 reviews\n",
      "read 9140 reviews\n",
      "read 9150 reviews\n",
      "read 9160 reviews\n",
      "read 9170 reviews\n",
      "read 9180 reviews\n",
      "read 9190 reviews\n",
      "read 9200 reviews\n",
      "read 9210 reviews\n",
      "read 9220 reviews\n",
      "read 9230 reviews\n",
      "read 9240 reviews\n",
      "read 9250 reviews\n",
      "read 9260 reviews\n",
      "read 9270 reviews\n",
      "read 9280 reviews\n",
      "read 9290 reviews\n",
      "read 9300 reviews\n",
      "read 9310 reviews\n",
      "read 9320 reviews\n",
      "read 9330 reviews\n",
      "read 9340 reviews\n",
      "read 9350 reviews\n",
      "read 9360 reviews\n",
      "read 9370 reviews\n",
      "read 9380 reviews\n",
      "read 9390 reviews\n",
      "read 9400 reviews\n",
      "read 9410 reviews\n",
      "read 9420 reviews\n",
      "read 9430 reviews\n",
      "read 9440 reviews\n",
      "read 9450 reviews\n",
      "read 9460 reviews\n",
      "read 9470 reviews\n",
      "read 9480 reviews\n",
      "read 9490 reviews\n",
      "read 9500 reviews\n",
      "read 9510 reviews\n",
      "read 9520 reviews\n",
      "read 9530 reviews\n",
      "read 9540 reviews\n",
      "read 9550 reviews\n",
      "read 9560 reviews\n",
      "read 9570 reviews\n",
      "read 9580 reviews\n",
      "read 9590 reviews\n",
      "read 9600 reviews\n",
      "read 9610 reviews\n",
      "read 9620 reviews\n",
      "read 9630 reviews\n",
      "read 9640 reviews\n",
      "read 9650 reviews\n",
      "read 9660 reviews\n",
      "read 9670 reviews\n",
      "read 9680 reviews\n",
      "read 9690 reviews\n",
      "read 9700 reviews\n",
      "read 9710 reviews\n",
      "read 9720 reviews\n",
      "read 9730 reviews\n",
      "read 9740 reviews\n",
      "read 9750 reviews\n",
      "read 9760 reviews\n",
      "read 9770 reviews\n",
      "read 9780 reviews\n",
      "read 9790 reviews\n",
      "read 9800 reviews\n",
      "read 9810 reviews\n",
      "read 9820 reviews\n",
      "read 9830 reviews\n",
      "read 9840 reviews\n",
      "read 9850 reviews\n",
      "read 9860 reviews\n",
      "read 9870 reviews\n",
      "read 9880 reviews\n",
      "read 9890 reviews\n",
      "read 9900 reviews\n",
      "read 9910 reviews\n",
      "read 9920 reviews\n",
      "read 9930 reviews\n",
      "read 9940 reviews\n",
      "read 9950 reviews\n",
      "read 9960 reviews\n",
      "read 9970 reviews\n",
      "read 9980 reviews\n",
      "read 9990 reviews\n",
      "read 10000 reviews\n",
      "read 10010 reviews\n",
      "read 10020 reviews\n",
      "read 10030 reviews\n",
      "read 10040 reviews\n",
      "read 10050 reviews\n",
      "read 10060 reviews\n",
      "read 10070 reviews\n",
      "read 10080 reviews\n",
      "read 10090 reviews\n",
      "read 10100 reviews\n",
      "read 10110 reviews\n",
      "read 10120 reviews\n",
      "read 10130 reviews\n",
      "read 10140 reviews\n",
      "read 10150 reviews\n",
      "read 10160 reviews\n",
      "read 10170 reviews\n",
      "read 10180 reviews\n",
      "read 10190 reviews\n",
      "read 10200 reviews\n",
      "read 10210 reviews\n",
      "read 10220 reviews\n",
      "read 10230 reviews\n",
      "read 10240 reviews\n",
      "read 10250 reviews\n",
      "read 10260 reviews\n",
      "read 10270 reviews\n",
      "read 10280 reviews\n",
      "read 10290 reviews\n",
      "read 10300 reviews\n",
      "read 10310 reviews\n",
      "read 10320 reviews\n",
      "read 10330 reviews\n",
      "read 10340 reviews\n",
      "read 10350 reviews\n",
      "read 10360 reviews\n",
      "read 10370 reviews\n",
      "read 10380 reviews\n",
      "read 10390 reviews\n",
      "read 10400 reviews\n",
      "read 10410 reviews\n",
      "read 10420 reviews\n",
      "read 10430 reviews\n",
      "read 10440 reviews\n",
      "read 10450 reviews\n",
      "read 10460 reviews\n",
      "read 10470 reviews\n",
      "read 10480 reviews\n",
      "read 10490 reviews\n",
      "read 10500 reviews\n",
      "read 10510 reviews\n",
      "read 10520 reviews\n",
      "read 10530 reviews\n",
      "read 10540 reviews\n",
      "read 10550 reviews\n",
      "read 10560 reviews\n",
      "read 10570 reviews\n",
      "read 10580 reviews\n",
      "read 10590 reviews\n",
      "read 10600 reviews\n",
      "read 10610 reviews\n",
      "read 10620 reviews\n",
      "read 10630 reviews\n",
      "read 10640 reviews\n",
      "read 10650 reviews\n",
      "read 10660 reviews\n",
      "read 10670 reviews\n",
      "read 10680 reviews\n",
      "read 10690 reviews\n",
      "read 10700 reviews\n",
      "read 10710 reviews\n",
      "read 10720 reviews\n",
      "read 10730 reviews\n",
      "read 10740 reviews\n",
      "read 10750 reviews\n",
      "read 10760 reviews\n",
      "read 10770 reviews\n",
      "read 10780 reviews\n",
      "read 10790 reviews\n",
      "read 10800 reviews\n",
      "read 10810 reviews\n",
      "read 10820 reviews\n",
      "read 10830 reviews\n",
      "read 10840 reviews\n",
      "read 10850 reviews\n",
      "read 10860 reviews\n",
      "read 10870 reviews\n",
      "read 10880 reviews\n",
      "read 10890 reviews\n",
      "read 10900 reviews\n",
      "read 10910 reviews\n",
      "read 10920 reviews\n",
      "read 10930 reviews\n",
      "read 10940 reviews\n",
      "read 10950 reviews\n",
      "read 10960 reviews\n",
      "read 10970 reviews\n",
      "read 10980 reviews\n",
      "read 10990 reviews\n",
      "read 11000 reviews\n",
      "read 11010 reviews\n",
      "read 11020 reviews\n",
      "read 11030 reviews\n",
      "read 11040 reviews\n",
      "read 11050 reviews\n",
      "read 11060 reviews\n",
      "read 11070 reviews\n",
      "read 11080 reviews\n",
      "read 11090 reviews\n",
      "read 11100 reviews\n",
      "read 11110 reviews\n",
      "read 11120 reviews\n",
      "read 11130 reviews\n",
      "read 11140 reviews\n",
      "read 11150 reviews\n",
      "read 11160 reviews\n",
      "read 11170 reviews\n",
      "read 11180 reviews\n",
      "read 11190 reviews\n",
      "read 11200 reviews\n",
      "read 11210 reviews\n",
      "read 11220 reviews\n",
      "read 11230 reviews\n",
      "read 11240 reviews\n",
      "read 11250 reviews\n",
      "read 11260 reviews\n",
      "read 11270 reviews\n",
      "read 11280 reviews\n",
      "read 11290 reviews\n",
      "read 11300 reviews\n",
      "read 11310 reviews\n",
      "read 11320 reviews\n",
      "read 11330 reviews\n",
      "read 11340 reviews\n",
      "read 11350 reviews\n",
      "read 11360 reviews\n",
      "read 11370 reviews\n",
      "read 11380 reviews\n",
      "read 11390 reviews\n",
      "read 11400 reviews\n",
      "read 11410 reviews\n",
      "read 11420 reviews\n",
      "read 11430 reviews\n",
      "read 11440 reviews\n",
      "read 11450 reviews\n",
      "read 11460 reviews\n",
      "read 11470 reviews\n",
      "read 11480 reviews\n",
      "read 11490 reviews\n",
      "read 11500 reviews\n",
      "read 11510 reviews\n",
      "read 11520 reviews\n",
      "read 11530 reviews\n",
      "read 11540 reviews\n",
      "read 11550 reviews\n",
      "read 11560 reviews\n",
      "read 11570 reviews\n",
      "read 11580 reviews\n",
      "read 11590 reviews\n",
      "read 11600 reviews\n",
      "read 11610 reviews\n",
      "read 11620 reviews\n",
      "read 11630 reviews\n",
      "read 11640 reviews\n",
      "read 11650 reviews\n",
      "read 11660 reviews\n",
      "read 11670 reviews\n",
      "read 11680 reviews\n",
      "read 11690 reviews\n",
      "read 11700 reviews\n",
      "read 11710 reviews\n",
      "read 11720 reviews\n",
      "read 11730 reviews\n",
      "read 11740 reviews\n",
      "read 11750 reviews\n",
      "read 11760 reviews\n",
      "read 11770 reviews\n",
      "read 11780 reviews\n",
      "read 11790 reviews\n",
      "read 11800 reviews\n",
      "read 11810 reviews\n",
      "read 11820 reviews\n",
      "read 11830 reviews\n",
      "read 11840 reviews\n",
      "read 11850 reviews\n",
      "read 11860 reviews\n",
      "read 11870 reviews\n",
      "read 11880 reviews\n",
      "read 11890 reviews\n",
      "read 11900 reviews\n",
      "read 11910 reviews\n",
      "read 11920 reviews\n",
      "read 11930 reviews\n",
      "read 11940 reviews\n",
      "read 11950 reviews\n",
      "read 11960 reviews\n",
      "read 11970 reviews\n",
      "read 11980 reviews\n",
      "read 11990 reviews\n",
      "read 12000 reviews\n",
      "read 12010 reviews\n",
      "read 12020 reviews\n",
      "read 12030 reviews\n",
      "read 12040 reviews\n",
      "read 12050 reviews\n",
      "read 12060 reviews\n",
      "read 12070 reviews\n",
      "read 12080 reviews\n",
      "read 12090 reviews\n",
      "read 12100 reviews\n",
      "read 12110 reviews\n",
      "read 12120 reviews\n",
      "read 12130 reviews\n",
      "read 12140 reviews\n",
      "read 12150 reviews\n",
      "read 12160 reviews\n",
      "read 12170 reviews\n",
      "read 12180 reviews\n",
      "read 12190 reviews\n",
      "read 12200 reviews\n",
      "read 12210 reviews\n",
      "read 12220 reviews\n",
      "read 12230 reviews\n",
      "read 12240 reviews\n",
      "read 12250 reviews\n",
      "read 12260 reviews\n",
      "read 12270 reviews\n",
      "read 12280 reviews\n",
      "read 12290 reviews\n",
      "read 12300 reviews\n",
      "read 12310 reviews\n",
      "read 12320 reviews\n",
      "read 12330 reviews\n",
      "read 12340 reviews\n",
      "read 12350 reviews\n",
      "read 12360 reviews\n",
      "read 12370 reviews\n",
      "read 12380 reviews\n",
      "read 12390 reviews\n",
      "read 12400 reviews\n",
      "read 12410 reviews\n",
      "read 12420 reviews\n",
      "read 12430 reviews\n",
      "read 12440 reviews\n",
      "read 12450 reviews\n",
      "read 12460 reviews\n",
      "read 12470 reviews\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 12480 reviews\n",
      "read 12490 reviews\n",
      "read 12500 reviews\n",
      "read 12510 reviews\n",
      "read 12520 reviews\n",
      "read 12530 reviews\n",
      "read 12540 reviews\n",
      "read 12550 reviews\n",
      "read 12560 reviews\n",
      "read 12570 reviews\n",
      "read 12580 reviews\n",
      "read 12590 reviews\n",
      "read 12600 reviews\n",
      "read 12610 reviews\n",
      "read 12620 reviews\n",
      "read 12630 reviews\n",
      "read 12640 reviews\n",
      "read 12650 reviews\n",
      "read 12660 reviews\n",
      "read 12670 reviews\n",
      "read 12680 reviews\n",
      "read 12690 reviews\n",
      "read 12700 reviews\n",
      "read 12710 reviews\n",
      "read 12720 reviews\n",
      "read 12730 reviews\n",
      "read 12740 reviews\n",
      "read 12750 reviews\n",
      "read 12760 reviews\n",
      "read 12770 reviews\n",
      "read 12780 reviews\n",
      "read 12790 reviews\n",
      "read 12800 reviews\n",
      "read 12810 reviews\n",
      "read 12820 reviews\n",
      "read 12830 reviews\n",
      "read 12840 reviews\n",
      "read 12850 reviews\n",
      "read 12860 reviews\n",
      "read 12870 reviews\n",
      "read 12880 reviews\n",
      "read 12890 reviews\n",
      "read 12900 reviews\n",
      "read 12910 reviews\n",
      "read 12920 reviews\n",
      "read 12930 reviews\n",
      "read 12940 reviews\n",
      "read 12950 reviews\n",
      "read 12960 reviews\n",
      "read 12970 reviews\n",
      "read 12980 reviews\n",
      "read 12990 reviews\n",
      "read 13000 reviews\n",
      "read 13010 reviews\n",
      "read 13020 reviews\n",
      "read 13030 reviews\n",
      "read 13040 reviews\n",
      "read 13050 reviews\n",
      "read 13060 reviews\n",
      "read 13070 reviews\n",
      "read 13080 reviews\n",
      "read 13090 reviews\n",
      "read 13100 reviews\n",
      "read 13110 reviews\n",
      "read 13120 reviews\n",
      "read 13130 reviews\n",
      "read 13140 reviews\n",
      "read 13150 reviews\n",
      "read 13160 reviews\n",
      "read 13170 reviews\n",
      "read 13180 reviews\n",
      "read 13190 reviews\n",
      "read 13200 reviews\n",
      "read 13210 reviews\n",
      "read 13220 reviews\n",
      "read 13230 reviews\n",
      "read 13240 reviews\n",
      "read 13250 reviews\n",
      "read 13260 reviews\n",
      "read 13270 reviews\n",
      "read 13280 reviews\n",
      "read 13290 reviews\n",
      "read 13300 reviews\n",
      "read 13310 reviews\n",
      "read 13320 reviews\n",
      "read 13330 reviews\n",
      "read 13340 reviews\n",
      "read 13350 reviews\n",
      "read 13360 reviews\n",
      "read 13370 reviews\n",
      "read 13380 reviews\n",
      "read 13390 reviews\n",
      "read 13400 reviews\n",
      "read 13410 reviews\n",
      "read 13420 reviews\n",
      "read 13430 reviews\n",
      "read 13440 reviews\n",
      "read 13450 reviews\n",
      "read 13460 reviews\n",
      "read 13470 reviews\n",
      "read 13480 reviews\n",
      "read 13490 reviews\n",
      "read 13500 reviews\n",
      "read 13510 reviews\n",
      "read 13520 reviews\n",
      "read 13530 reviews\n",
      "read 13540 reviews\n",
      "read 13550 reviews\n",
      "read 13560 reviews\n",
      "read 13570 reviews\n",
      "read 13580 reviews\n",
      "read 13590 reviews\n",
      "read 13600 reviews\n",
      "read 13610 reviews\n",
      "read 13620 reviews\n",
      "read 13630 reviews\n",
      "read 13640 reviews\n",
      "read 13650 reviews\n",
      "read 13660 reviews\n",
      "read 13670 reviews\n",
      "read 13680 reviews\n",
      "read 13690 reviews\n",
      "read 13700 reviews\n",
      "read 13710 reviews\n",
      "read 13720 reviews\n",
      "read 13730 reviews\n",
      "read 13740 reviews\n",
      "read 13750 reviews\n",
      "read 13760 reviews\n",
      "read 13770 reviews\n",
      "read 13780 reviews\n",
      "read 13790 reviews\n",
      "read 13800 reviews\n",
      "read 13810 reviews\n",
      "read 13820 reviews\n",
      "read 13830 reviews\n",
      "read 13840 reviews\n",
      "read 13850 reviews\n",
      "read 13860 reviews\n",
      "read 13870 reviews\n",
      "read 13880 reviews\n",
      "read 13890 reviews\n",
      "read 13900 reviews\n",
      "read 13910 reviews\n",
      "read 13920 reviews\n",
      "read 13930 reviews\n",
      "read 13940 reviews\n",
      "read 13950 reviews\n",
      "read 13960 reviews\n",
      "read 13970 reviews\n",
      "read 13980 reviews\n",
      "read 13990 reviews\n",
      "read 14000 reviews\n",
      "read 14010 reviews\n",
      "read 14020 reviews\n",
      "read 14030 reviews\n",
      "read 14040 reviews\n",
      "read 14050 reviews\n",
      "read 14060 reviews\n",
      "read 14070 reviews\n",
      "read 14080 reviews\n",
      "read 14090 reviews\n",
      "read 14100 reviews\n",
      "read 14110 reviews\n",
      "read 14120 reviews\n",
      "read 14130 reviews\n",
      "read 14140 reviews\n",
      "read 14150 reviews\n",
      "read 14160 reviews\n",
      "read 14170 reviews\n",
      "read 14180 reviews\n",
      "read 14190 reviews\n",
      "read 14200 reviews\n",
      "read 14210 reviews\n",
      "read 14220 reviews\n",
      "read 14230 reviews\n",
      "read 14240 reviews\n",
      "read 14250 reviews\n",
      "read 14260 reviews\n",
      "read 14270 reviews\n",
      "read 14280 reviews\n",
      "read 14290 reviews\n",
      "read 14300 reviews\n",
      "read 14310 reviews\n",
      "read 14320 reviews\n",
      "read 14330 reviews\n",
      "read 14340 reviews\n",
      "read 14350 reviews\n",
      "read 14360 reviews\n",
      "read 14370 reviews\n",
      "read 14380 reviews\n",
      "read 14390 reviews\n",
      "read 14400 reviews\n",
      "read 14410 reviews\n",
      "read 14420 reviews\n",
      "read 14430 reviews\n",
      "read 14440 reviews\n",
      "read 14450 reviews\n",
      "read 14460 reviews\n",
      "read 14470 reviews\n",
      "read 14480 reviews\n",
      "read 14490 reviews\n",
      "read 14500 reviews\n",
      "read 14510 reviews\n",
      "read 14520 reviews\n",
      "read 14530 reviews\n",
      "read 14540 reviews\n",
      "read 14550 reviews\n",
      "read 14560 reviews\n",
      "read 14570 reviews\n",
      "read 14580 reviews\n",
      "read 14590 reviews\n",
      "read 14600 reviews\n",
      "read 14610 reviews\n",
      "read 14620 reviews\n",
      "read 14630 reviews\n",
      "read 14640 reviews\n",
      "read 14650 reviews\n",
      "read 14660 reviews\n",
      "read 14670 reviews\n",
      "read 14680 reviews\n",
      "read 14690 reviews\n",
      "read 14700 reviews\n",
      "read 14710 reviews\n",
      "read 14720 reviews\n",
      "read 14730 reviews\n",
      "read 14740 reviews\n",
      "read 14750 reviews\n",
      "read 14760 reviews\n",
      "read 14770 reviews\n",
      "read 14780 reviews\n",
      "read 14790 reviews\n",
      "read 14800 reviews\n",
      "read 14810 reviews\n",
      "read 14820 reviews\n",
      "read 14830 reviews\n",
      "read 14840 reviews\n",
      "read 14850 reviews\n",
      "read 14860 reviews\n",
      "read 14870 reviews\n",
      "read 14880 reviews\n",
      "read 14890 reviews\n",
      "read 14900 reviews\n",
      "read 14910 reviews\n",
      "read 14920 reviews\n",
      "read 14930 reviews\n",
      "read 14940 reviews\n",
      "read 14950 reviews\n",
      "read 14960 reviews\n",
      "read 14970 reviews\n",
      "read 14980 reviews\n",
      "read 14990 reviews\n",
      "read 15000 reviews\n",
      "read 15010 reviews\n",
      "read 15020 reviews\n",
      "read 15030 reviews\n",
      "read 15040 reviews\n",
      "read 15050 reviews\n",
      "read 15060 reviews\n",
      "read 15070 reviews\n",
      "read 15080 reviews\n",
      "read 15090 reviews\n",
      "read 15100 reviews\n",
      "read 15110 reviews\n",
      "read 15120 reviews\n",
      "read 15130 reviews\n",
      "read 15140 reviews\n",
      "read 15150 reviews\n",
      "read 15160 reviews\n",
      "read 15170 reviews\n",
      "read 15180 reviews\n",
      "read 15190 reviews\n",
      "read 15200 reviews\n",
      "read 15210 reviews\n",
      "read 15220 reviews\n",
      "read 15230 reviews\n",
      "read 15240 reviews\n",
      "read 15250 reviews\n",
      "read 15260 reviews\n",
      "read 15270 reviews\n",
      "read 15280 reviews\n",
      "read 15290 reviews\n",
      "read 15300 reviews\n",
      "read 15310 reviews\n",
      "read 15320 reviews\n",
      "read 15330 reviews\n",
      "read 15340 reviews\n",
      "read 15350 reviews\n",
      "read 15360 reviews\n",
      "read 15370 reviews\n",
      "read 15380 reviews\n",
      "read 15390 reviews\n",
      "read 15400 reviews\n",
      "read 15410 reviews\n",
      "read 15420 reviews\n",
      "read 15430 reviews\n",
      "read 15440 reviews\n",
      "read 15450 reviews\n",
      "read 15460 reviews\n",
      "read 15470 reviews\n",
      "read 15480 reviews\n",
      "read 15490 reviews\n",
      "read 15500 reviews\n",
      "read 15510 reviews\n",
      "read 15520 reviews\n",
      "read 15530 reviews\n",
      "read 15540 reviews\n",
      "read 15550 reviews\n",
      "read 15560 reviews\n",
      "read 15570 reviews\n",
      "read 15580 reviews\n",
      "read 15590 reviews\n",
      "read 15600 reviews\n",
      "read 15610 reviews\n",
      "read 15620 reviews\n",
      "read 15630 reviews\n",
      "read 15640 reviews\n",
      "read 15650 reviews\n",
      "read 15660 reviews\n",
      "read 15670 reviews\n",
      "read 15680 reviews\n",
      "read 15690 reviews\n",
      "read 15700 reviews\n",
      "read 15710 reviews\n",
      "read 15720 reviews\n",
      "read 15730 reviews\n",
      "read 15740 reviews\n",
      "read 15750 reviews\n",
      "read 15760 reviews\n",
      "read 15770 reviews\n",
      "read 15780 reviews\n",
      "read 15790 reviews\n",
      "read 15800 reviews\n",
      "read 15810 reviews\n",
      "read 15820 reviews\n",
      "read 15830 reviews\n",
      "read 15840 reviews\n",
      "read 15850 reviews\n",
      "read 15860 reviews\n",
      "read 15870 reviews\n",
      "read 15880 reviews\n",
      "read 15890 reviews\n",
      "read 15900 reviews\n",
      "read 15910 reviews\n",
      "read 15920 reviews\n",
      "read 15930 reviews\n",
      "read 15940 reviews\n",
      "read 15950 reviews\n",
      "read 15960 reviews\n",
      "read 15970 reviews\n",
      "read 15980 reviews\n",
      "read 15990 reviews\n",
      "read 16000 reviews\n",
      "read 16010 reviews\n",
      "read 16020 reviews\n",
      "read 16030 reviews\n",
      "read 16040 reviews\n",
      "read 16050 reviews\n",
      "read 16060 reviews\n",
      "read 16070 reviews\n",
      "read 16080 reviews\n",
      "read 16090 reviews\n",
      "read 16100 reviews\n",
      "read 16110 reviews\n",
      "read 16120 reviews\n",
      "read 16130 reviews\n",
      "read 16140 reviews\n",
      "read 16150 reviews\n",
      "read 16160 reviews\n",
      "read 16170 reviews\n",
      "read 16180 reviews\n",
      "read 16190 reviews\n",
      "read 16200 reviews\n",
      "read 16210 reviews\n",
      "read 16220 reviews\n",
      "read 16230 reviews\n",
      "read 16240 reviews\n",
      "read 16250 reviews\n",
      "read 16260 reviews\n",
      "read 16270 reviews\n",
      "read 16280 reviews\n",
      "read 16290 reviews\n",
      "read 16300 reviews\n",
      "read 16310 reviews\n",
      "read 16320 reviews\n",
      "read 16330 reviews\n",
      "read 16340 reviews\n",
      "read 16350 reviews\n",
      "read 16360 reviews\n",
      "read 16370 reviews\n",
      "read 16380 reviews\n",
      "read 16390 reviews\n",
      "read 16400 reviews\n",
      "read 16410 reviews\n",
      "read 16420 reviews\n",
      "read 16430 reviews\n",
      "read 16440 reviews\n",
      "read 16450 reviews\n",
      "read 16460 reviews\n",
      "read 16470 reviews\n",
      "read 16480 reviews\n",
      "read 16490 reviews\n",
      "read 16500 reviews\n",
      "read 16510 reviews\n",
      "read 16520 reviews\n",
      "read 16530 reviews\n",
      "read 16540 reviews\n",
      "read 16550 reviews\n",
      "read 16560 reviews\n",
      "read 16570 reviews\n",
      "read 16580 reviews\n",
      "read 16590 reviews\n",
      "read 16600 reviews\n",
      "read 16610 reviews\n",
      "read 16620 reviews\n",
      "read 16630 reviews\n",
      "read 16640 reviews\n",
      "read 16650 reviews\n",
      "read 16660 reviews\n",
      "read 16670 reviews\n",
      "read 16680 reviews\n",
      "read 16690 reviews\n",
      "read 16700 reviews\n",
      "read 16710 reviews\n",
      "read 16720 reviews\n",
      "read 16730 reviews\n",
      "read 16740 reviews\n",
      "read 16750 reviews\n",
      "read 16760 reviews\n",
      "read 16770 reviews\n",
      "read 16780 reviews\n",
      "read 16790 reviews\n",
      "read 16800 reviews\n",
      "read 16810 reviews\n",
      "read 16820 reviews\n",
      "read 16830 reviews\n",
      "read 16840 reviews\n",
      "read 16850 reviews\n",
      "read 16860 reviews\n",
      "read 16870 reviews\n",
      "read 16880 reviews\n",
      "read 16890 reviews\n",
      "read 16900 reviews\n",
      "read 16910 reviews\n",
      "read 16920 reviews\n",
      "read 16930 reviews\n",
      "read 16940 reviews\n",
      "read 16950 reviews\n",
      "read 16960 reviews\n",
      "read 16970 reviews\n",
      "read 16980 reviews\n",
      "read 16990 reviews\n",
      "read 17000 reviews\n",
      "read 17010 reviews\n",
      "read 17020 reviews\n",
      "read 17030 reviews\n",
      "read 17040 reviews\n",
      "read 17050 reviews\n",
      "read 17060 reviews\n",
      "read 17070 reviews\n",
      "read 17080 reviews\n",
      "read 17090 reviews\n",
      "read 17100 reviews\n",
      "read 17110 reviews\n",
      "read 17120 reviews\n",
      "read 17130 reviews\n",
      "read 17140 reviews\n",
      "read 17150 reviews\n",
      "read 17160 reviews\n",
      "read 17170 reviews\n",
      "read 17180 reviews\n",
      "read 17190 reviews\n",
      "read 17200 reviews\n",
      "read 17210 reviews\n",
      "read 17220 reviews\n",
      "read 17230 reviews\n",
      "read 17240 reviews\n",
      "read 17250 reviews\n",
      "read 17260 reviews\n",
      "read 17270 reviews\n",
      "read 17280 reviews\n",
      "read 17290 reviews\n",
      "read 17300 reviews\n",
      "read 17310 reviews\n",
      "read 17320 reviews\n",
      "read 17330 reviews\n",
      "read 17340 reviews\n",
      "read 17350 reviews\n",
      "read 17360 reviews\n",
      "read 17370 reviews\n",
      "read 17380 reviews\n",
      "read 17390 reviews\n",
      "read 17400 reviews\n",
      "read 17410 reviews\n",
      "read 17420 reviews\n",
      "read 17430 reviews\n",
      "read 17440 reviews\n",
      "read 17450 reviews\n",
      "read 17460 reviews\n",
      "read 17470 reviews\n",
      "read 17480 reviews\n",
      "read 17490 reviews\n",
      "read 17500 reviews\n",
      "read 17510 reviews\n",
      "read 17520 reviews\n",
      "read 17530 reviews\n",
      "read 17540 reviews\n",
      "read 17550 reviews\n",
      "read 17560 reviews\n",
      "read 17570 reviews\n",
      "read 17580 reviews\n",
      "read 17590 reviews\n",
      "read 17600 reviews\n",
      "read 17610 reviews\n",
      "read 17620 reviews\n",
      "read 17630 reviews\n",
      "read 17640 reviews\n",
      "read 17650 reviews\n",
      "read 17660 reviews\n",
      "read 17670 reviews\n",
      "read 17680 reviews\n",
      "read 17690 reviews\n",
      "read 17700 reviews\n",
      "read 17710 reviews\n",
      "read 17720 reviews\n",
      "read 17730 reviews\n",
      "read 17740 reviews\n",
      "read 17750 reviews\n",
      "read 17760 reviews\n",
      "read 17770 reviews\n",
      "read 17780 reviews\n",
      "read 17790 reviews\n",
      "read 17800 reviews\n",
      "read 17810 reviews\n",
      "read 17820 reviews\n",
      "read 17830 reviews\n",
      "read 17840 reviews\n",
      "read 17850 reviews\n",
      "read 17860 reviews\n",
      "read 17870 reviews\n",
      "read 17880 reviews\n",
      "read 17890 reviews\n",
      "read 17900 reviews\n",
      "read 17910 reviews\n",
      "read 17920 reviews\n",
      "read 17930 reviews\n",
      "read 17940 reviews\n",
      "read 17950 reviews\n",
      "read 17960 reviews\n",
      "read 17970 reviews\n",
      "read 17980 reviews\n",
      "read 17990 reviews\n",
      "read 18000 reviews\n",
      "read 18010 reviews\n",
      "read 18020 reviews\n",
      "read 18030 reviews\n",
      "read 18040 reviews\n",
      "read 18050 reviews\n",
      "read 18060 reviews\n",
      "read 18070 reviews\n",
      "read 18080 reviews\n",
      "read 18090 reviews\n",
      "read 18100 reviews\n",
      "read 18110 reviews\n",
      "read 18120 reviews\n",
      "read 18130 reviews\n",
      "read 18140 reviews\n",
      "read 18150 reviews\n",
      "read 18160 reviews\n",
      "read 18170 reviews\n",
      "read 18180 reviews\n",
      "read 18190 reviews\n",
      "read 18200 reviews\n",
      "read 18210 reviews\n",
      "read 18220 reviews\n",
      "read 18230 reviews\n",
      "read 18240 reviews\n",
      "read 18250 reviews\n",
      "read 18260 reviews\n",
      "read 18270 reviews\n",
      "read 18280 reviews\n",
      "read 18290 reviews\n",
      "read 18300 reviews\n",
      "read 18310 reviews\n",
      "read 18320 reviews\n",
      "read 18330 reviews\n",
      "read 18340 reviews\n",
      "read 18350 reviews\n",
      "read 18360 reviews\n",
      "read 18370 reviews\n",
      "read 18380 reviews\n",
      "read 18390 reviews\n",
      "read 18400 reviews\n",
      "read 18410 reviews\n",
      "read 18420 reviews\n",
      "read 18430 reviews\n",
      "read 18440 reviews\n",
      "read 18450 reviews\n",
      "read 18460 reviews\n",
      "read 18470 reviews\n",
      "read 18480 reviews\n",
      "read 18490 reviews\n",
      "read 18500 reviews\n",
      "read 18510 reviews\n",
      "read 18520 reviews\n",
      "read 18530 reviews\n",
      "read 18540 reviews\n",
      "read 18550 reviews\n",
      "read 18560 reviews\n",
      "read 18570 reviews\n",
      "read 18580 reviews\n",
      "read 18590 reviews\n",
      "read 18600 reviews\n",
      "read 18610 reviews\n",
      "read 18620 reviews\n",
      "read 18630 reviews\n",
      "read 18640 reviews\n",
      "read 18650 reviews\n",
      "read 18660 reviews\n",
      "read 18670 reviews\n",
      "read 18680 reviews\n",
      "read 18690 reviews\n",
      "read 18700 reviews\n",
      "read 18710 reviews\n",
      "read 18720 reviews\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 18730 reviews\n",
      "read 18740 reviews\n",
      "read 18750 reviews\n",
      "read 18760 reviews\n",
      "read 18770 reviews\n",
      "read 18780 reviews\n",
      "read 18790 reviews\n",
      "read 18800 reviews\n",
      "read 18810 reviews\n",
      "read 18820 reviews\n",
      "read 18830 reviews\n",
      "read 18840 reviews\n",
      "read 18850 reviews\n",
      "read 18860 reviews\n",
      "read 18870 reviews\n",
      "read 18880 reviews\n",
      "read 18890 reviews\n",
      "read 18900 reviews\n",
      "read 18910 reviews\n",
      "read 18920 reviews\n",
      "read 18930 reviews\n",
      "read 18940 reviews\n",
      "read 18950 reviews\n",
      "read 18960 reviews\n",
      "read 18970 reviews\n",
      "read 18980 reviews\n",
      "read 18990 reviews\n",
      "read 19000 reviews\n",
      "read 19010 reviews\n",
      "read 19020 reviews\n",
      "read 19030 reviews\n",
      "read 19040 reviews\n",
      "read 19050 reviews\n",
      "read 19060 reviews\n",
      "read 19070 reviews\n",
      "read 19080 reviews\n",
      "read 19090 reviews\n",
      "read 19100 reviews\n",
      "read 19110 reviews\n",
      "read 19120 reviews\n",
      "read 19130 reviews\n",
      "read 19140 reviews\n",
      "read 19150 reviews\n",
      "read 19160 reviews\n",
      "read 19170 reviews\n",
      "read 19180 reviews\n",
      "read 19190 reviews\n",
      "read 19200 reviews\n",
      "read 19210 reviews\n",
      "read 19220 reviews\n",
      "read 19230 reviews\n",
      "read 19240 reviews\n",
      "read 19250 reviews\n",
      "read 19260 reviews\n",
      "read 19270 reviews\n",
      "read 19280 reviews\n",
      "read 19290 reviews\n",
      "read 19300 reviews\n",
      "read 19310 reviews\n",
      "read 19320 reviews\n",
      "read 19330 reviews\n",
      "read 19340 reviews\n",
      "read 19350 reviews\n",
      "read 19360 reviews\n",
      "read 19370 reviews\n",
      "read 19380 reviews\n",
      "read 19390 reviews\n",
      "read 19400 reviews\n",
      "read 19410 reviews\n",
      "read 19420 reviews\n",
      "read 19430 reviews\n",
      "read 19440 reviews\n",
      "read 19450 reviews\n",
      "read 19460 reviews\n",
      "read 19470 reviews\n",
      "read 19480 reviews\n",
      "read 19490 reviews\n",
      "read 19500 reviews\n",
      "read 19510 reviews\n",
      "read 19520 reviews\n",
      "read 19530 reviews\n",
      "read 19540 reviews\n",
      "read 19550 reviews\n",
      "read 19560 reviews\n",
      "read 19570 reviews\n",
      "read 19580 reviews\n",
      "read 19590 reviews\n",
      "read 19600 reviews\n",
      "read 19610 reviews\n",
      "read 19620 reviews\n",
      "read 19630 reviews\n",
      "read 19640 reviews\n",
      "read 19650 reviews\n",
      "read 19660 reviews\n",
      "read 19670 reviews\n",
      "read 19680 reviews\n",
      "read 19690 reviews\n",
      "read 19700 reviews\n",
      "read 19710 reviews\n",
      "read 19720 reviews\n",
      "read 19730 reviews\n",
      "read 19740 reviews\n",
      "read 19750 reviews\n",
      "read 19760 reviews\n",
      "read 19770 reviews\n",
      "read 19780 reviews\n",
      "read 19790 reviews\n",
      "read 19800 reviews\n",
      "read 19810 reviews\n",
      "read 19820 reviews\n",
      "read 19830 reviews\n",
      "read 19840 reviews\n",
      "read 19850 reviews\n",
      "read 19860 reviews\n",
      "read 19870 reviews\n",
      "read 19880 reviews\n",
      "read 19890 reviews\n",
      "read 19900 reviews\n",
      "read 19910 reviews\n",
      "read 19920 reviews\n",
      "read 19930 reviews\n",
      "read 19940 reviews\n",
      "read 19950 reviews\n",
      "read 19960 reviews\n",
      "read 19970 reviews\n",
      "read 19980 reviews\n",
      "read 19990 reviews\n",
      "read 20000 reviews\n",
      "read 20010 reviews\n",
      "read 20020 reviews\n",
      "read 20030 reviews\n",
      "read 20040 reviews\n",
      "read 20050 reviews\n",
      "read 20060 reviews\n",
      "read 20070 reviews\n",
      "read 20080 reviews\n",
      "read 20090 reviews\n",
      "read 20100 reviews\n",
      "read 20110 reviews\n",
      "read 20120 reviews\n",
      "read 20130 reviews\n",
      "read 20140 reviews\n",
      "read 20150 reviews\n",
      "read 20160 reviews\n",
      "read 20170 reviews\n",
      "read 20180 reviews\n",
      "read 20190 reviews\n",
      "read 20200 reviews\n",
      "read 20210 reviews\n",
      "read 20220 reviews\n",
      "read 20230 reviews\n",
      "read 20240 reviews\n",
      "read 20250 reviews\n",
      "read 20260 reviews\n",
      "read 20270 reviews\n",
      "read 20280 reviews\n",
      "read 20290 reviews\n",
      "read 20300 reviews\n",
      "read 20310 reviews\n",
      "read 20320 reviews\n",
      "read 20330 reviews\n",
      "read 20340 reviews\n",
      "read 20350 reviews\n",
      "read 20360 reviews\n",
      "read 20370 reviews\n",
      "read 20380 reviews\n",
      "read 20390 reviews\n",
      "read 20400 reviews\n",
      "read 20410 reviews\n",
      "read 20420 reviews\n",
      "read 20430 reviews\n",
      "read 20440 reviews\n",
      "read 20450 reviews\n",
      "read 20460 reviews\n",
      "read 20470 reviews\n",
      "read 20480 reviews\n",
      "read 20490 reviews\n",
      "read 20500 reviews\n",
      "read 20510 reviews\n",
      "read 20520 reviews\n",
      "read 20530 reviews\n",
      "read 20540 reviews\n",
      "read 20550 reviews\n",
      "read 20560 reviews\n",
      "read 20570 reviews\n",
      "read 20580 reviews\n",
      "read 20590 reviews\n",
      "read 20600 reviews\n",
      "read 20610 reviews\n",
      "read 20620 reviews\n",
      "read 20630 reviews\n",
      "read 20640 reviews\n",
      "read 20650 reviews\n",
      "read 20660 reviews\n",
      "read 20670 reviews\n",
      "read 20680 reviews\n",
      "read 20690 reviews\n",
      "read 20700 reviews\n",
      "read 20710 reviews\n",
      "read 20720 reviews\n",
      "read 20730 reviews\n",
      "read 20740 reviews\n",
      "read 20750 reviews\n",
      "read 20760 reviews\n",
      "read 20770 reviews\n",
      "read 20780 reviews\n",
      "read 20790 reviews\n",
      "read 20800 reviews\n",
      "read 20810 reviews\n",
      "read 20820 reviews\n",
      "read 20830 reviews\n",
      "read 20840 reviews\n",
      "read 20850 reviews\n",
      "read 20860 reviews\n",
      "read 20870 reviews\n",
      "read 20880 reviews\n",
      "read 20890 reviews\n",
      "read 20900 reviews\n",
      "read 20910 reviews\n",
      "read 20920 reviews\n",
      "read 20930 reviews\n",
      "read 20940 reviews\n",
      "read 20950 reviews\n",
      "read 20960 reviews\n",
      "read 20970 reviews\n",
      "read 20980 reviews\n",
      "read 20990 reviews\n",
      "read 21000 reviews\n",
      "read 21010 reviews\n",
      "read 21020 reviews\n",
      "read 21030 reviews\n",
      "read 21040 reviews\n",
      "read 21050 reviews\n",
      "read 21060 reviews\n",
      "read 21070 reviews\n",
      "read 21080 reviews\n",
      "read 21090 reviews\n",
      "read 21100 reviews\n",
      "read 21110 reviews\n",
      "read 21120 reviews\n",
      "read 21130 reviews\n",
      "read 21140 reviews\n",
      "read 21150 reviews\n",
      "read 21160 reviews\n",
      "read 21170 reviews\n",
      "read 21180 reviews\n",
      "read 21190 reviews\n",
      "read 21200 reviews\n",
      "read 21210 reviews\n",
      "read 21220 reviews\n",
      "read 21230 reviews\n",
      "read 21240 reviews\n",
      "read 21250 reviews\n",
      "read 21260 reviews\n",
      "read 21270 reviews\n",
      "read 21280 reviews\n",
      "read 21290 reviews\n",
      "read 21300 reviews\n",
      "read 21310 reviews\n",
      "read 21320 reviews\n",
      "read 21330 reviews\n",
      "read 21340 reviews\n",
      "read 21350 reviews\n",
      "read 21360 reviews\n",
      "read 21370 reviews\n",
      "read 21380 reviews\n",
      "read 21390 reviews\n",
      "read 21400 reviews\n",
      "read 21410 reviews\n",
      "read 21420 reviews\n",
      "read 21430 reviews\n",
      "read 21440 reviews\n",
      "read 21450 reviews\n",
      "read 21460 reviews\n",
      "read 21470 reviews\n",
      "read 21480 reviews\n",
      "read 21490 reviews\n",
      "read 21500 reviews\n",
      "read 21510 reviews\n",
      "read 21520 reviews\n",
      "read 21530 reviews\n",
      "read 21540 reviews\n",
      "read 21550 reviews\n",
      "read 21560 reviews\n",
      "read 21570 reviews\n",
      "read 21580 reviews\n",
      "read 21590 reviews\n",
      "read 21600 reviews\n",
      "read 21610 reviews\n",
      "read 21620 reviews\n",
      "read 21630 reviews\n",
      "read 21640 reviews\n",
      "read 21650 reviews\n",
      "read 21660 reviews\n",
      "read 21670 reviews\n",
      "read 21680 reviews\n",
      "read 21690 reviews\n",
      "read 21700 reviews\n",
      "read 21710 reviews\n",
      "read 21720 reviews\n",
      "read 21730 reviews\n",
      "read 21740 reviews\n",
      "read 21750 reviews\n",
      "read 21760 reviews\n",
      "read 21770 reviews\n",
      "read 21780 reviews\n",
      "read 21790 reviews\n",
      "read 21800 reviews\n",
      "read 21810 reviews\n",
      "read 21820 reviews\n",
      "read 21830 reviews\n",
      "read 21840 reviews\n",
      "read 21850 reviews\n",
      "read 21860 reviews\n",
      "read 21870 reviews\n",
      "read 21880 reviews\n",
      "read 21890 reviews\n",
      "read 21900 reviews\n",
      "read 21910 reviews\n",
      "read 21920 reviews\n",
      "read 21930 reviews\n",
      "read 21940 reviews\n",
      "read 21950 reviews\n",
      "read 21960 reviews\n",
      "read 21970 reviews\n",
      "read 21980 reviews\n",
      "read 21990 reviews\n",
      "read 22000 reviews\n",
      "read 22010 reviews\n",
      "read 22020 reviews\n",
      "read 22030 reviews\n",
      "read 22040 reviews\n",
      "read 22050 reviews\n",
      "read 22060 reviews\n",
      "read 22070 reviews\n",
      "read 22080 reviews\n",
      "read 22090 reviews\n",
      "read 22100 reviews\n",
      "read 22110 reviews\n",
      "read 22120 reviews\n",
      "read 22130 reviews\n",
      "read 22140 reviews\n",
      "read 22150 reviews\n",
      "read 22160 reviews\n",
      "read 22170 reviews\n",
      "read 22180 reviews\n",
      "read 22190 reviews\n",
      "read 22200 reviews\n",
      "read 22210 reviews\n",
      "read 22220 reviews\n",
      "read 22230 reviews\n",
      "read 22240 reviews\n",
      "read 22250 reviews\n",
      "read 22260 reviews\n",
      "read 22270 reviews\n",
      "read 22280 reviews\n",
      "read 22290 reviews\n",
      "read 22300 reviews\n",
      "read 22310 reviews\n",
      "read 22320 reviews\n",
      "read 22330 reviews\n",
      "read 22340 reviews\n",
      "read 22350 reviews\n",
      "read 22360 reviews\n",
      "read 22370 reviews\n",
      "read 22380 reviews\n",
      "read 22390 reviews\n",
      "read 22400 reviews\n",
      "read 22410 reviews\n",
      "read 22420 reviews\n",
      "read 22430 reviews\n",
      "read 22440 reviews\n",
      "read 22450 reviews\n",
      "read 22460 reviews\n",
      "read 22470 reviews\n",
      "read 22480 reviews\n",
      "read 22490 reviews\n",
      "read 22500 reviews\n",
      "read 22510 reviews\n",
      "read 22520 reviews\n",
      "read 22530 reviews\n",
      "read 22540 reviews\n",
      "read 22550 reviews\n",
      "read 22560 reviews\n",
      "read 22570 reviews\n",
      "read 22580 reviews\n",
      "read 22590 reviews\n",
      "read 22600 reviews\n",
      "read 22610 reviews\n",
      "read 22620 reviews\n",
      "read 22630 reviews\n",
      "read 22640 reviews\n",
      "read 22650 reviews\n",
      "read 22660 reviews\n",
      "read 22670 reviews\n",
      "read 22680 reviews\n",
      "read 22690 reviews\n",
      "read 22700 reviews\n",
      "read 22710 reviews\n",
      "read 22720 reviews\n",
      "read 22730 reviews\n",
      "read 22740 reviews\n",
      "read 22750 reviews\n",
      "read 22760 reviews\n",
      "read 22770 reviews\n",
      "read 22780 reviews\n",
      "read 22790 reviews\n",
      "read 22800 reviews\n",
      "read 22810 reviews\n",
      "read 22820 reviews\n",
      "read 22830 reviews\n",
      "read 22840 reviews\n",
      "read 22850 reviews\n",
      "read 22860 reviews\n",
      "read 22870 reviews\n",
      "read 22880 reviews\n",
      "read 22890 reviews\n",
      "read 22900 reviews\n",
      "read 22910 reviews\n",
      "read 22920 reviews\n",
      "read 22930 reviews\n",
      "read 22940 reviews\n",
      "read 22950 reviews\n",
      "read 22960 reviews\n",
      "read 22970 reviews\n",
      "read 22980 reviews\n",
      "read 22990 reviews\n",
      "read 23000 reviews\n",
      "read 23010 reviews\n",
      "read 23020 reviews\n",
      "read 23030 reviews\n",
      "read 23040 reviews\n",
      "read 23050 reviews\n",
      "read 23060 reviews\n",
      "read 23070 reviews\n",
      "read 23080 reviews\n",
      "read 23090 reviews\n",
      "read 23100 reviews\n",
      "read 23110 reviews\n",
      "read 23120 reviews\n",
      "read 23130 reviews\n",
      "read 23140 reviews\n",
      "read 23150 reviews\n",
      "read 23160 reviews\n",
      "read 23170 reviews\n",
      "read 23180 reviews\n",
      "read 23190 reviews\n",
      "read 23200 reviews\n",
      "read 23210 reviews\n",
      "read 23220 reviews\n",
      "read 23230 reviews\n",
      "read 23240 reviews\n",
      "read 23250 reviews\n",
      "read 23260 reviews\n",
      "read 23270 reviews\n",
      "read 23280 reviews\n",
      "read 23290 reviews\n",
      "read 23300 reviews\n",
      "read 23310 reviews\n",
      "read 23320 reviews\n",
      "read 23330 reviews\n",
      "read 23340 reviews\n",
      "read 23350 reviews\n",
      "read 23360 reviews\n",
      "read 23370 reviews\n",
      "read 23380 reviews\n",
      "read 23390 reviews\n",
      "read 23400 reviews\n",
      "read 23410 reviews\n",
      "read 23420 reviews\n",
      "read 23430 reviews\n",
      "read 23440 reviews\n",
      "read 23450 reviews\n",
      "read 23460 reviews\n",
      "read 23470 reviews\n",
      "read 23480 reviews\n",
      "read 23490 reviews\n",
      "read 23500 reviews\n",
      "read 23510 reviews\n",
      "read 23520 reviews\n",
      "read 23530 reviews\n",
      "read 23540 reviews\n",
      "read 23550 reviews\n",
      "read 23560 reviews\n",
      "read 23570 reviews\n",
      "read 23580 reviews\n",
      "read 23590 reviews\n",
      "read 23600 reviews\n",
      "read 23610 reviews\n",
      "read 23620 reviews\n",
      "read 23630 reviews\n",
      "read 23640 reviews\n",
      "read 23650 reviews\n",
      "read 23660 reviews\n",
      "read 23670 reviews\n",
      "read 23680 reviews\n",
      "read 23690 reviews\n",
      "read 23700 reviews\n",
      "read 23710 reviews\n",
      "read 23720 reviews\n",
      "read 23730 reviews\n",
      "read 23740 reviews\n",
      "read 23750 reviews\n",
      "read 23760 reviews\n",
      "read 23770 reviews\n",
      "read 23780 reviews\n",
      "read 23790 reviews\n",
      "read 23800 reviews\n",
      "read 23810 reviews\n",
      "read 23820 reviews\n",
      "read 23830 reviews\n",
      "read 23840 reviews\n",
      "read 23850 reviews\n",
      "read 23860 reviews\n",
      "read 23870 reviews\n",
      "read 23880 reviews\n",
      "read 23890 reviews\n",
      "read 23900 reviews\n",
      "read 23910 reviews\n",
      "read 23920 reviews\n",
      "read 23930 reviews\n",
      "read 23940 reviews\n",
      "read 23950 reviews\n",
      "read 23960 reviews\n",
      "read 23970 reviews\n",
      "read 23980 reviews\n",
      "read 23990 reviews\n",
      "read 24000 reviews\n",
      "read 24010 reviews\n",
      "read 24020 reviews\n",
      "read 24030 reviews\n",
      "read 24040 reviews\n",
      "read 24050 reviews\n",
      "read 24060 reviews\n",
      "read 24070 reviews\n",
      "read 24080 reviews\n",
      "read 24090 reviews\n",
      "read 24100 reviews\n",
      "read 24110 reviews\n",
      "read 24120 reviews\n",
      "read 24130 reviews\n",
      "read 24140 reviews\n",
      "read 24150 reviews\n",
      "read 24160 reviews\n",
      "read 24170 reviews\n",
      "read 24180 reviews\n",
      "read 24190 reviews\n",
      "read 24200 reviews\n",
      "read 24210 reviews\n",
      "read 24220 reviews\n",
      "read 24230 reviews\n",
      "read 24240 reviews\n",
      "read 24250 reviews\n",
      "read 24260 reviews\n",
      "read 24270 reviews\n",
      "read 24280 reviews\n",
      "read 24290 reviews\n",
      "read 24300 reviews\n",
      "read 24310 reviews\n",
      "read 24320 reviews\n",
      "read 24330 reviews\n",
      "read 24340 reviews\n",
      "read 24350 reviews\n",
      "read 24360 reviews\n",
      "read 24370 reviews\n",
      "read 24380 reviews\n",
      "read 24390 reviews\n",
      "read 24400 reviews\n",
      "read 24410 reviews\n",
      "read 24420 reviews\n",
      "read 24430 reviews\n",
      "read 24440 reviews\n",
      "read 24450 reviews\n",
      "read 24460 reviews\n",
      "read 24470 reviews\n",
      "read 24480 reviews\n",
      "read 24490 reviews\n",
      "read 24500 reviews\n",
      "read 24510 reviews\n",
      "read 24520 reviews\n",
      "read 24530 reviews\n",
      "read 24540 reviews\n",
      "read 24550 reviews\n",
      "read 24560 reviews\n",
      "read 24570 reviews\n",
      "read 24580 reviews\n",
      "read 24590 reviews\n",
      "read 24600 reviews\n",
      "read 24610 reviews\n",
      "read 24620 reviews\n",
      "read 24630 reviews\n",
      "read 24640 reviews\n",
      "read 24650 reviews\n",
      "read 24660 reviews\n",
      "read 24670 reviews\n",
      "read 24680 reviews\n",
      "read 24690 reviews\n",
      "read 24700 reviews\n",
      "read 24710 reviews\n",
      "read 24720 reviews\n",
      "read 24730 reviews\n",
      "read 24740 reviews\n",
      "read 24750 reviews\n",
      "read 24760 reviews\n",
      "read 24770 reviews\n",
      "read 24780 reviews\n",
      "read 24790 reviews\n",
      "read 24800 reviews\n",
      "read 24810 reviews\n",
      "read 24820 reviews\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to oncology [('allelic', 0.8633838295936584), ('imbalance', 0.8616378307342529), ('myelogenous', 0.84666907787323), ('mrna', 0.841934323310852), ('expression', 0.8412970900535583), ('hereditary', 0.8280978202819824), ('urothelial', 0.8223068118095398), ('cancer', 0.8216057419776917), ('colorectal', 0.8202099800109863), ('gstp', 0.8124186396598816)]\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import spatial\n",
    "\n",
    "def show_file_contents(input_file):\n",
    "    with gzip.open(input_file, 'rb') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            print(line)\n",
    "            if (i==1000):\n",
    "                print(\"reached 100\")\n",
    "                print(\"read {0} reviews\".format(i))\n",
    "            break\n",
    "        \n",
    "    yield gensim.utils.simple_preprocess(line)\n",
    "            \n",
    "                \n",
    "                \n",
    " \n",
    "def read_input(input_file):\n",
    "    \n",
    "    print(\"method ivoked\")\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    with gzip.open(input_file, 'rb') as f:\n",
    "        print (\"opened file\")\n",
    "        for i, line in enumerate(f):\n",
    "\n",
    "            if (i % 10 == 0):\n",
    "                #print(\"getting dedauks\")\n",
    "                print(\"read {0} reviews\".format(i))\n",
    "            # do some pre-processing and return list of words for each review\n",
    "            # text\n",
    "            yield gensim.utils.simple_preprocess(line)\n",
    "\n",
    "index2word_set = set(model.wv.index2word)\n",
    "\n",
    "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "        #abspath = os.path.dirname(os.path.abspath(__file__))\n",
    "        data_file = os.path.join(\"Same_SIM_Master_List.csv.gz\")\n",
    "\n",
    "    # read the tokenized reviews into a list\n",
    "    # each review item becomes a serries of words\n",
    "    # so this becomes a list of lists\n",
    "        documents = list(read_input(data_file))\n",
    "        #print(documents )\n",
    "        model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=1, workers=10)\n",
    "        model.train(documents,total_examples=len(documents),epochs=10)\n",
    "        w1 = \"oncology\"\n",
    "        print(\"Most similar to {0}\".format(w1), model.wv.most_similar(positive=w1))\n",
    "        \n",
    "        s1_afv = avg_feature_vector('oncology (breast); mrna; gene expression profiling by real-time rt-pcr of 21 genes; utilizing formalin-fixed paraffin-embedded tissue; algorithm reported as recurrence score', model=model, num_features=150, index2word_set=index2word_set)\n",
    "        s2_afv = avg_feature_vector('oncology (breast); mrna; gene expression profiling by real-time rt-pcr of 21 genes; utilizing formalin-fixed paraffin-embedded tissue; algorithm reported as recurrence score', model=model, num_features=150, index2word_set=index2word_set)\n",
    "        sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\n",
    "        print(sim)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method ivoked\n",
      "opened file\n",
      "24827\n",
      "[TaggedDocument(words=['dysphagia'], tags=['0']), TaggedDocument(words=['screening'], tags=['1'])]\n",
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:579: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "iteration 40\n",
      "iteration 41\n",
      "iteration 42\n",
      "iteration 43\n",
      "iteration 44\n",
      "iteration 45\n",
      "iteration 46\n",
      "iteration 47\n",
      "iteration 48\n",
      "iteration 49\n",
      "iteration 50\n",
      "iteration 51\n",
      "iteration 52\n",
      "iteration 53\n",
      "iteration 54\n",
      "iteration 55\n",
      "iteration 56\n",
      "iteration 57\n",
      "iteration 58\n",
      "iteration 59\n",
      "iteration 60\n",
      "iteration 61\n",
      "iteration 62\n",
      "iteration 63\n",
      "iteration 64\n",
      "iteration 65\n",
      "iteration 66\n",
      "iteration 67\n",
      "iteration 68\n",
      "iteration 69\n",
      "iteration 70\n",
      "iteration 71\n",
      "iteration 72\n",
      "iteration 73\n",
      "iteration 74\n",
      "iteration 75\n",
      "iteration 76\n",
      "iteration 77\n",
      "iteration 78\n",
      "iteration 79\n",
      "iteration 80\n",
      "iteration 81\n",
      "iteration 82\n",
      "iteration 83\n",
      "iteration 84\n",
      "iteration 85\n",
      "iteration 86\n",
      "iteration 87\n",
      "iteration 88\n",
      "iteration 89\n",
      "iteration 90\n",
      "iteration 91\n",
      "iteration 92\n",
      "iteration 93\n",
      "iteration 94\n",
      "iteration 95\n",
      "iteration 96\n",
      "iteration 97\n",
      "iteration 98\n",
      "iteration 99\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "def read_input(input_file):\n",
    "    \n",
    "    print(\"method ivoked\")\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    with gzip.open(input_file, 'rb') as f:\n",
    "        print (\"opened file\")\n",
    "        for i, line in enumerate(f):\n",
    "\n",
    "            if (i % 10 == 0):\n",
    "                #print(\"getting dedauks\")\n",
    "                \"read {0} reviews\".format(i)\n",
    "            # do some pre-processing and return list of words for each review\n",
    "            # text\n",
    "            yield gensim.utils.simple_preprocess(line)\n",
    "\n",
    "data_file = os.path.join(\"Same_SIM_Master_List.csv.gz\")\n",
    "data = list(read_input(data_file))\n",
    "print(len(data))\n",
    "#data = [\"oncology (breast); mrna; gene expression profiling by real-time rt-pcr of 21 genes; utilizing formalin-fixed paraffin-embedded tissue; algorithm reported as recurrence score\",\n",
    "        #\"flt3 (fms-related tyrosine kinase 3) (eg; acute myeloid leukemia); gene analysis; internal tandem duplication (itd) variants (ie; exons 14; 15)\",\n",
    "        #\"npm1 (nucleophosmin) (eg; acute myeloid leukemia) gene analysis; quantitative\",\n",
    "        #\"oncology (colon); mrna; gene expression profiling by real-time rt-pcr of 12 genes (7 content and 5 housekeeping); utilizing formalin-fixed paraffin-embedded tissue; algorithm reported as a recurrence score\",\"they chat really well\",\"he is running out of time for chatting\",\"i am having cup of tea\"]\n",
    "\n",
    "for idata in data:   \n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(_d), tags=[str(i)]) for i, _d in enumerate(idata)]\n",
    "\n",
    "    \n",
    "print(tagged_data)\n",
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d1v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Doc2Vec.load(\"d1v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', 0.26953816413879395), ('1', -0.2932582497596741)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = word_tokenize(\"dysphagia page\")\n",
    "\n",
    "new_vector = model.infer_vector(tokens)\n",
    "sims = model.docvecs.most_similar(positive=[model.infer_vector(tokens)],topn=3)\n",
    "print (sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:579: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:61: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "iteration 40\n",
      "iteration 41\n",
      "iteration 42\n",
      "iteration 43\n",
      "iteration 44\n",
      "iteration 45\n",
      "iteration 46\n",
      "iteration 47\n",
      "iteration 48\n",
      "iteration 49\n",
      "iteration 50\n",
      "iteration 51\n",
      "iteration 52\n",
      "iteration 53\n",
      "iteration 54\n",
      "iteration 55\n",
      "iteration 56\n",
      "iteration 57\n",
      "iteration 58\n",
      "iteration 59\n",
      "iteration 60\n",
      "iteration 61\n",
      "iteration 62\n",
      "iteration 63\n",
      "iteration 64\n",
      "iteration 65\n",
      "iteration 66\n",
      "iteration 67\n",
      "iteration 68\n",
      "iteration 69\n",
      "iteration 70\n",
      "iteration 71\n",
      "iteration 72\n",
      "iteration 73\n",
      "iteration 74\n",
      "iteration 75\n",
      "iteration 76\n",
      "iteration 77\n",
      "iteration 78\n",
      "iteration 79\n",
      "iteration 80\n",
      "iteration 81\n",
      "iteration 82\n",
      "iteration 83\n",
      "iteration 84\n",
      "iteration 85\n",
      "iteration 86\n",
      "iteration 87\n",
      "iteration 88\n",
      "iteration 89\n",
      "iteration 90\n",
      "iteration 91\n",
      "iteration 92\n",
      "iteration 93\n",
      "iteration 94\n",
      "iteration 95\n",
      "iteration 96\n",
      "iteration 97\n",
      "iteration 98\n",
      "iteration 99\n"
     ]
    }
   ],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# numpy\n",
    "import numpy\n",
    "\n",
    "# random\n",
    "from random import shuffle\n",
    "\n",
    "# classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences\n",
    "    \n",
    "sources = {'Same_SIM_Master_List.csv':'TEST_NEG'}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)\n",
    "\n",
    "sent = sentences.to_array()\n",
    "\n",
    "print(len(sent))\n",
    "\n",
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)\n",
    "\n",
    "model.build_vocab(sentences.to_array())\n",
    "\n",
    "for epoch in range(100):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(sentences.to_array(),\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Lable.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('TEST_NEG_17203', 0.8862501978874207), ('TEST_NEG_8575', 0.8798203468322754), ('TEST_NEG_4575', 0.8773929476737976), ('TEST_NEG_16888', 0.8762221932411194), ('TEST_NEG_17563', 0.8737908601760864)]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TEST_NEG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-429cba93220b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEST_NEG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m12203\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'TEST_NEG' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "model = Doc2Vec.load('Lable.model')\n",
    "tokens = word_tokenize(\"oncology Breast\")\n",
    "new_vector = model.infer_vector(tokens)\n",
    "sims = model.docvecs.most_similar(positive=[model.infer_vector(tokens)],topn=5)\n",
    "print (sims)\n",
    "\n",
    "print(TEST_NEG[12203])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import pandas as pd\n",
    "\n",
    "mylist = list()\n",
    "resultFrame = pd.DataFrame(columns=('NewCode','NewCodeDesc','SameSimCode', 'CPTDesc','Accuracy'),index=range(60000))\n",
    "rownum = -1\n",
    "NewCodeNum = -1\n",
    "\n",
    "def create_tokenizer_score(new_series, train_series, tokenizer):\n",
    "    \"\"\"\n",
    "    return the tf idf score of each possible pairs of documents\n",
    "    Args:\n",
    "        new_series (pd.Series): new data (To compare against train data)\n",
    "        train_series (pd.Series): train data (To fit the tf-idf transformer)\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    train_tfidf = tokenizer.fit_transform(train_series)\n",
    "    \n",
    "    new_tfidf = tokenizer.transform(new_series)\n",
    "\n",
    "    X = pd.DataFrame(cosine_similarity(new_tfidf, train_tfidf), columns=train_series.index)\n",
    "    X['ix_new'] = new_series.index\n",
    "    score = pd.melt(\n",
    "        X,\n",
    "        id_vars='ix_new',\n",
    "        var_name='ix_train',\n",
    "        value_name='score'\n",
    "    )\n",
    "    return score\n",
    "\n",
    "\n",
    "def Get_MasterCodeDesc():\n",
    "    df = pd.read_csv(\"Same_SIM_Master_List.csv\")\n",
    "    sMasterCodeDesc = df['CPTLongDesc'].tolist()\n",
    "    return sMasterCodeDesc;\n",
    "\n",
    "def GetNewCPT():\n",
    "    NewCodeFrame = pd.read_csv(\"NewCodes.csv\")\n",
    "    sNewCode = NewCodeFrame['HCPCS'].tolist()\n",
    "    return sNewCode;\n",
    "\n",
    "def GetNewCPTDesc():\n",
    "    NewCodeFrame = pd.read_csv(\"NewCodes.csv\")\n",
    "    sNewCodeDesc = NewCodeFrame['LongDesc'].tolist()\n",
    "    return sNewCodeDesc;\n",
    "\n",
    "def GetMasterCPTCode(Val,NewCode,NewCodeDesc,Accuracy,x):\n",
    "    df = pd.read_csv(\"Same_SIM_Master_List.csv\")    \n",
    "    GetCPTValue = df.iloc[Val,0]\n",
    "    GetCPTDesc = df.iloc[Val,1]    \n",
    "    mylist.append(GetCPTValue)\n",
    "    mylist.append(GetCPTDesc)\n",
    "    resultFrame.loc[x].NewCode = NewCode\n",
    "    resultFrame.loc[x].SameSimCode = GetCPTValue\n",
    "    resultFrame.loc[x].CPTDesc = GetCPTDesc\n",
    "    resultFrame.loc[x].NewCodeDesc = NewCodeDesc\n",
    "    resultFrame.loc[x].Accuracy = Accuracy\n",
    "    return mylist;\n",
    "\n",
    "#train_CPT_Set = pd.Series(GetMasterCPT())\n",
    "train_set = pd.Series(Get_MasterCodeDesc())\n",
    "NewCodeDescSet = pd.Series(GetNewCPTDesc())\n",
    "NewCodeSet = pd.Series(GetNewCPT())\n",
    "for i in NewCodeDescSet:\n",
    "    NewCodeNum = NewCodeNum+1\n",
    "    test_set = pd.Series(i)\n",
    "    tokenizer = TfidfVectorizer(stop_words = 'english') # initiate here your own tokenizer (TfidfVectorizer, CountVectorizer, with stopwords...)\n",
    "    score = create_tokenizer_score(train_series=train_set, new_series=test_set, tokenizer=tokenizer)\n",
    "    #score\n",
    "    pd.set_option(\"display.max_rows\", 11)\n",
    "    sortedscore = score.sort_values(by=['score'], ascending=False)\n",
    "    sortedscore.head(11)\n",
    "    for x in range(0,10):\n",
    "        rownum = rownum+1\n",
    "        svalue = sortedscore.iloc[x,1]\n",
    "        Accuracy = sortedscore.iloc[x,2]\n",
    "        result = GetMasterCPTCode(svalue,NewCodeSet[NewCodeNum],i,Accuracy*100,rownum)\n",
    " \n",
    "resultFrame\n",
    "\n",
    "export_csv = resultFrame.to_csv ('export_dataframe3.csv', index = None, header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
