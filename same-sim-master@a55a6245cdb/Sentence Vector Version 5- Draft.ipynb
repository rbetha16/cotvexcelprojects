{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0045U\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "TfidfVectorizer - Vocabulary wasn't fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-52b627518fdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mrowFound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresultFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresultFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNewCode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mNewCodeSet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mNewCodeNum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2083\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2084\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2085\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-52b627518fdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;31m#tokenizer = TfidfVectorizer(stop_words = 'english')# initiate here your own tokenizer (TfidfVectorizer, CountVectorizer, with stopwords...)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_tokenizer_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_series\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_series\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m         \u001b[1;31m#score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"display.max_rows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-52b627518fdd>\u001b[0m in \u001b[0;36mcreate_tokenizer_score\u001b[1;34m(new_series, train_series, tokenizer)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mlsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_tfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mterms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcomp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mget_feature_names\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    958\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m         \u001b[1;34m\"\"\"Array mapping from feature integer indices to feature name\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m         return [t for t, i in sorted(six.iteritems(self.vocabulary_),\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[1;34m\"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%(name)s - Vocabulary wasn't fitted.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vocabulary_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 768\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: TfidfVectorizer - Vocabulary wasn't fitted."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from nltk import word_tokenize\n",
    "\n",
    "mylist = list()\n",
    "InputSheet = \"Same-Sim_SAMPLE.csv\"\n",
    "MasterSheet = \"Same_SIM_Master_List.csv\"\n",
    "resultFrame = pd.DataFrame(columns=('NewCode','NewCodeDesc','SameSimCode', 'CPTDesc','Accuracy','Rank','Comments'),index=range(30000))\n",
    "rownum = -1\n",
    "NewCodeNum = -1\n",
    "\n",
    "def create_tokenizer_score(new_series, train_series, tokenizer):\n",
    "    \"\"\"\n",
    "    return the tf idf score of each possible pairs of documents\n",
    "    Args:\n",
    "        new_series (pd.Series): new data (To compare against train data)\n",
    "        train_series (pd.Series): train data (To fit the tf-idf transformer)\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    train_tfidf = tokenizer.fit_transform(train_series)\n",
    "    \n",
    "    new_tfidf = tokenizer.transform(new_series)\n",
    "\n",
    "    X = pd.DataFrame(cosine_similarity(new_tfidf, train_tfidf), columns=train_series.index)\n",
    "    X['ix_new'] = new_series.index\n",
    "    score = pd.melt(\n",
    "        X,\n",
    "        id_vars='ix_new',\n",
    "        var_name='ix_train',\n",
    "        value_name='score'\n",
    "    )\n",
    "    return score\n",
    "\n",
    "\n",
    "def Get_MasterCodeDesc():\n",
    "    df = pd.read_csv(MasterSheet)\n",
    "    sMasterCodeDesc = df['CPTLongDesc'].tolist()\n",
    "    return sMasterCodeDesc;\n",
    "\n",
    "def Get_SimCodeDesc():\n",
    "    df = pd.read_csv(InputSheet,encoding ='latin1')\n",
    "    sMasterCodeDesc = df['Sim_LongDesc'].tolist()\n",
    "    return sMasterCodeDesc;\n",
    "\n",
    "def GetNewCPT():\n",
    "    NewCodeFrame = pd.read_csv(InputSheet,encoding ='latin1')\n",
    "    sNewCode = NewCodeFrame['HCPCS'].tolist()\n",
    "    return sNewCode;\n",
    "\n",
    "def GetNewCPTDesc():\n",
    "    NewCodeFrame = pd.read_csv(InputSheet,encoding ='latin1')\n",
    "    sNewCodeDesc = NewCodeFrame['LongDesc'].tolist()\n",
    "    return sNewCodeDesc;\n",
    "\n",
    "def GetSameSIMData():\n",
    "    SameSimDf = pd.read_csv(InputSheet,encoding ='latin1')\n",
    "    sNewCodeDesc = SameSimDf['SimCode'].tolist()\n",
    "    return sNewCodeDesc;\n",
    "   \n",
    "def textblob_tokenizer(str_input):\n",
    "    blob = TextBlob(str_input.lower())\n",
    "    tokens = blob.words\n",
    "    words = [token.stem() for token in tokens]\n",
    "    return words\n",
    "\n",
    "def GetRowNum(): \n",
    "    try:\n",
    "        rownum = resultFrame.loc[(resultFrame.Accuracy == '0003T') & (resultFrame.Rank == 'CERVICOGRAPY')].index[0]\n",
    "            #print(rownum)\n",
    "        resultFrame.loc[rownum,'Comments'] = \"Test\"\n",
    "    except:\n",
    "        \"No record found\"\n",
    "\n",
    "def GetMasterCPTCode(Val,NewCode,NewCodeDesc,Accuracy,RankNo,Comment,x):\n",
    "    df = pd.read_csv(MasterSheet)    \n",
    "    GetCPTValue = df.iloc[Val,0]\n",
    "    GetCPTDesc = df.iloc[Val,1]    \n",
    "    mylist.append(GetCPTValue)\n",
    "    mylist.append(GetCPTDesc)\n",
    "    resultFrame.loc[x].NewCode = NewCode\n",
    "    resultFrame.loc[x].SameSimCode = GetCPTValue\n",
    "    resultFrame.loc[x].CPTDesc = GetCPTDesc\n",
    "    resultFrame.loc[x].NewCodeDesc = NewCodeDesc\n",
    "    resultFrame.loc[x].Accuracy = Accuracy\n",
    "    resultFrame.loc[x].Rank = RankNo\n",
    "    resultFrame.loc[x].Comments = Comment\n",
    "    return mylist;\n",
    "\n",
    "#train_CPT_Set = pd.Series(GetMasterCPT())\n",
    "train_set = pd.Series(Get_MasterCodeDesc())\n",
    "SimCodeDescSet = pd.Series(Get_SimCodeDesc())\n",
    "NewCodeDescSet = pd.Series(GetNewCPTDesc())\n",
    "NewCodeSet = pd.Series(GetNewCPT())\n",
    "SameSimSet = pd.Series(GetSameSIMData())\n",
    "for i in NewCodeDescSet:\n",
    "    NewCodeNum = NewCodeNum+1\n",
    "    test_set = pd.Series(i)   \n",
    "    try:\n",
    "        rowFound = resultFrame.loc[(resultFrame.NewCode == NewCodeSet[NewCodeNum])].index[0]  \n",
    "        try:        \n",
    "            rowNumSameSimFound = resultFrame.loc[(resultFrame.NewCode == NewCodeSet[NewCodeNum]) & (resultFrame.SameSimCode == SameSimSet[NewCodeNum])].index[0]\n",
    "            resultFrame.loc[rowNumSameSimFound,'Comments'] = \"Present Both in MD and Autoproposal\"\n",
    "        except:\n",
    "            try:\n",
    "                rownum = rownum+1\n",
    "                resultFrame.loc[rownum].NewCode = NewCodeSet[NewCodeNum]\n",
    "                resultFrame.loc[rownum].NewCodeDesc = NewCodeDescSet[NewCodeNum]\n",
    "                resultFrame.loc[rownum].SameSimCode = SameSimSet[NewCodeNum]\n",
    "                resultFrame.loc[rownum].CPTDesc = SimCodeDescSet[NewCodeNum]\n",
    "                resultFrame.loc[rownum].Comments = \"Present only by MD\"\n",
    "            except:\n",
    "                \"Nothing\"\n",
    "    except:\n",
    "        print(NewCodeSet[NewCodeNum])\n",
    "        #print(i)\n",
    "        test_set2 = word_tokenize(i)\n",
    "        #tokenizer = TfidfVectorizer(stop_words = 'english')# initiate here your own tokenizer (TfidfVectorizer, CountVectorizer, with stopwords...)\n",
    "        tokenizer = TfidfVectorizer()\n",
    "        score = create_tokenizer_score(train_series=train_set, new_series=test_set, tokenizer=tokenizer)\n",
    "        #score\n",
    "        pd.set_option(\"display.max_rows\", 11)\n",
    "        sortedscore = score.sort_values(by=['score'], ascending=False)\n",
    "        sortedscore.head(11)\n",
    "        ranknum = 0\n",
    "        found = \"false\"\n",
    "        for x in range(0,10):\n",
    "            ranknum = ranknum+1\n",
    "            df = pd.read_csv(MasterSheet) \n",
    "            svalue = sortedscore.iloc[x,1]\n",
    "            GetCPTValue = df.iloc[svalue,0]\n",
    "            if (GetCPTValue == SameSimSet[NewCodeNum]):\n",
    "                found = \"true\"\n",
    "                break \n",
    "\n",
    "        RecNum =0\n",
    "        for x in range(0,10):\n",
    "            rownum = rownum+1\n",
    "            svalue = sortedscore.iloc[x,1]\n",
    "            Accuracy = sortedscore.iloc[x,2]\n",
    "            \n",
    "            Comments = \"Present in Autoproposal\"\n",
    "\n",
    "            if (RecNum ==0):\n",
    "                result = GetMasterCPTCode(svalue,NewCodeSet[NewCodeNum],i,Accuracy*100,ranknum,Comments,rownum)\n",
    "            else:\n",
    "                result = GetMasterCPTCode(svalue,NewCodeSet[NewCodeNum],i,Accuracy*100,\"\",Comments,rownum)\n",
    "            RecNum =RecNum +1\n",
    "           \n",
    "        try:         \n",
    "            rowNumSameSimFound = resultFrame.loc[(resultFrame.NewCode == NewCodeSet[NewCodeNum]) & (resultFrame.SameSimCode == SameSimSet[NewCodeNum])].index[0]\n",
    "            resultFrame.loc[rowNumSameSimFound,'Comments'] = \"Present Both in MD and Autoproposal\"\n",
    "        except:\n",
    "            try:\n",
    "                rownum = rownum+1 \n",
    "                resultFrame.loc[rownum].NewCode = NewCodeSet[NewCodeNum]\n",
    "                resultFrame.loc[rownum].NewCodeDesc = NewCodeDescSet[NewCodeNum]\n",
    "                resultFrame.loc[rownum].SameSimCode = SameSimSet[NewCodeNum]\n",
    "                resultFrame.loc[rownum].CPTDesc = SimCodeDescSet[NewCodeNum]\n",
    "                resultFrame.loc[rownum].Comments = \"Present only by MD\"\n",
    "            except:\n",
    "                \"Nothing\"\n",
    "\n",
    "#export_csv = resultFrame.to_csv ('export_dataframe_V4_2018.csv', index = None, header=True)\n",
    "\n",
    "resultFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "SameSimDf = pd.read_csv(\"Same-Sim_SAMPLE.csv\",encoding ='latin1')\n",
    "sNewCodeDesc = SameSimDf['Sim_LongDesc'].tolist()\n",
    "\n",
    "# combine documents\n",
    "#doc_complete = [doc1, doc2, doc3, doc4, doc5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =vectorizer.fit_transform(sNewCodeDesc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "analysis\n",
      "or\n",
      "sequence\n",
      "variants\n",
      "performed\n",
      "and\n",
      "eg\n",
      "gene\n",
      "copy\n",
      "dna\n",
      "for\n",
      "genomic\n",
      "interrogation\n",
      "neoplasm\n",
      "organ\n",
      "rearrangements\n",
      "solid\n",
      "targeted\n",
      "number\n",
      "npm1\n",
      " \n",
      "Concept 1:\n",
      "algorithm\n",
      "embedded\n",
      "expression\n",
      "fixed\n",
      "formalin\n",
      "mrna\n",
      "oncology\n",
      "paraffin\n",
      "pcr\n",
      "profiling\n",
      "real\n",
      "rt\n",
      "score\n",
      "time\n",
      "tissue\n",
      "as\n",
      "reported\n",
      "of\n",
      "utilizing\n",
      "by\n",
      " \n",
      "Concept 2:\n",
      "ms\n",
      "drug\n",
      "lc\n",
      "of\n",
      "gc\n",
      "date\n",
      "per\n",
      "service\n",
      "any\n",
      "chromatography\n",
      "or\n",
      "all\n",
      "an\n",
      "comparison\n",
      "compounds\n",
      "drugs\n",
      "estimated\n",
      "evaluation\n",
      "fluid\n",
      "including\n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa = TruncatedSVD(n_components=3,n_iter=100)\n",
    "lsa.fit(X)\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "for i,comp in enumerate(lsa.components_):\n",
    "    termsInComp = zip(terms,comp)\n",
    "    sortedterms = sorted(termsInComp, key=lambda x: x[1],reverse=True)[:20]\n",
    "    print(\"Concept %d:\" % i)\n",
    "    for term in sortedterms:\n",
    "        print(term[0])\n",
    "    print(\" \")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max_df corresponds to < documents than min_df",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-3565a53f4246>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    104\u001b[0m      \u001b[1;31m#fit LSA model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m      \u001b[0mlsa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m      \u001b[0mlsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_term_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_df_prop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m      \u001b[1;31m#find k most similar terms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-3565a53f4246>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, corpus, min_term_freq, max_df_prop, n_components)\u001b[0m\n\u001b[0;32m     55\u001b[0m      \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_term_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_df_prop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_term_freq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_df_prop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m           \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m           \u001b[0msvd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m           \u001b[0mlsa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m         \"\"\"\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    884\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmax_doc_count\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmin_doc_count\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m                 raise ValueError(\n\u001b[1;32m--> 886\u001b[1;33m                     \"max_df corresponds to < documents than min_df\")\n\u001b[0m\u001b[0;32m    887\u001b[0m             X, self.stop_words_ = self._limit_features(X, vocabulary,\n\u001b[0;32m    888\u001b[0m                                                        \u001b[0mmax_doc_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: max_df corresponds to < documents than min_df"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import string\n",
    "          \n",
    "class NormalizeText:\n",
    "    \n",
    "     def __init__(self):\n",
    "          self.trans_table = self.get_no_punctuation_table()\n",
    "          self.PorterStemmer = PorterStemmer()\n",
    "\n",
    "     def get_no_punctuation_table(self,translate_to=None):\n",
    "          not_letters_or_digits = unicode(string.punctuation)+'1234567890'\n",
    "          translate_table = dict((ord(char), translate_to) for char in not_letters_or_digits)\n",
    "          return translate_table    \n",
    "     \n",
    "\n",
    "     def normalize(self,text):\n",
    "          orig_text = text\n",
    "          text = unicode(text)\n",
    "          trans_table = self.trans_table\n",
    "          text = text.translate(trans_table)\n",
    "          text = text.lower()\n",
    "          text = nltk.word_tokenize(text)\n",
    "          text = [self.PorterStemmer.stem(word) for word in text if not word in stopwords.words('english')]\n",
    "          text = \" \".join(text)\n",
    "          return text\n",
    "     \n",
    "\n",
    "class MyCorpus:\n",
    "     def __init__(self, filepath, delim=','):\n",
    "          self.filepath = filepath\n",
    "          self.delim = delim\n",
    "     \n",
    "     def __iter__(self): #each line assumed to be a separate document\n",
    "          for line in open(self.filepath):\n",
    "               yield line.split(self.delim)[1]\n",
    "\n",
    "\n",
    "class LSA:\n",
    "     def __init__(self):\n",
    "          pass\n",
    "\n",
    "     #singular value decomposition on the term-document matrix\n",
    "     def fit(self,corpus,min_term_freq,max_df_prop,n_components):\n",
    "          self.transformer = TfidfVectorizer(min_df=int(min_term_freq),max_df=float(max_df_prop))\n",
    "          tfidf = self.transformer.fit_transform(corpus) \n",
    "          svd = TruncatedSVD(n_components = n_components)\n",
    "          lsa = svd.fit_transform(tfidf.T)\n",
    "          self.model = lsa\n",
    "\n",
    "     \n",
    "     \n",
    "class Similarity:\n",
    "     def __init__(self,transformer,model):\n",
    "          self.transformer = transformer\n",
    "          self.model = model\n",
    "          self.norm = NormalizeText()\n",
    "     \n",
    "     #cosine similarity\n",
    "     def cos_sim(self,X_col,Y_col):\n",
    "          dot_prod = np.dot(X_col.T,Y_col)\n",
    "          X_norm = np.sqrt(np.dot(X_col.T,X_col))\n",
    "          Y_norm = np.sqrt(np.dot(Y_col.T,Y_col))\n",
    "          cos = dot_prod/(X_norm*Y_norm)\n",
    "          return cos\n",
    "     \n",
    "     def k_closest_terms(self,k,term):\n",
    "          term = self.norm.normalize(term)\n",
    "          index = self.transformer.vocabulary_[term]\n",
    "          \n",
    "          terms = self.transformer.get_feature_names()\n",
    "          closestTerms = pd.Series(index=terms)\n",
    "          \n",
    "          for i in xrange(len(self.model)):\n",
    "               closestTerms.loc[terms[i]] = self.cos_sim(self.model[index,:].T,self.model[i,:].T)\n",
    "               \n",
    "          closestTerms.sort_values(ascending=False,inplace=True)\n",
    "                      \n",
    "          return closestTerms.index[0:k].tolist()          \n",
    "          \n",
    "     \n",
    "     \n",
    "if __name__==\"__main__\":\n",
    "     corpus_file = \"Same-Sim_SAMPLE.csv\"\n",
    "min_term_freq = 10 #minimum number of times the word must appear in the corpus\n",
    "     max_df_prop = 0.10 #maximum proportion of documents in which the word appears\n",
    "     n_components = 200 #number of components for dimensionality reduction\n",
    "     \n",
    "     \n",
    "     #initialize corpus\n",
    "     corpus = MyCorpus(corpus_file) #each line in corpus file assumed to be separate document\n",
    "     \n",
    "     #fit LSA model\n",
    "     lsa = LSA()\n",
    "     lsa.fit(corpus, min_term_freq, max_df_prop,n_components)\n",
    "     \n",
    "     #find k most similar terms\n",
    "     term = 'happiness'\n",
    "     k = 50\n",
    "     similarity = Similarity(lsa.transformer,lsa.model)\n",
    "     k_closest_terms = similarity.k_closest_terms(k=k,term=term)\n",
    "     print (k_closest_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
